{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81ee0f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported.\n"
     ]
    }
   ],
   "source": [
    "# @title Import necessary libraries\n",
    "import os\n",
    "import asyncio\n",
    "from google.adk.agents import Agent\n",
    "from google.adk.models.lite_llm import LiteLlm # For multi-model support\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "from google.adk.runners import Runner\n",
    "from google.genai import types # For creating message Content/Parts\n",
    "\n",
    "import warnings\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "print(\"Libraries imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b69c3a12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a499e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Keys Set:\n",
      "Google API Key set: Yes\n",
      "OpenAI API Key set: Yes\n",
      "Anthropic API Key set: Yes\n"
     ]
    }
   ],
   "source": [
    "# @title Configure API Keys (Replace with your actual keys!)\n",
    "\n",
    "# --- Verify Keys (Optional Check) ---\n",
    "print(\"API Keys Set:\")\n",
    "print(f\"Google API Key set: {'Yes' if os.environ.get('GOOGLE_API_KEY') and os.environ['GOOGLE_API_KEY'] != 'YOUR_GOOGLE_API_KEY' else 'No (REPLACE PLACEHOLDER!)'}\")\n",
    "print(f\"OpenAI API Key set: {'Yes' if os.environ.get('OPENAI_API_KEY') and os.environ['OPENAI_API_KEY'] != 'YOUR_OPENAI_API_KEY' else 'No (REPLACE PLACEHOLDER!)'}\")\n",
    "print(f\"Anthropic API Key set: {'Yes' if os.environ.get('ANTHROPIC_API_KEY') and os.environ['ANTHROPIC_API_KEY'] != 'YOUR_ANTHROPIC_API_KEY' else 'No (REPLACE PLACEHOLDER!)'}\")\n",
    "\n",
    "# Configure ADK to use API keys directly (not Vertex AI for this multi-model setup)\n",
    "os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = \"False\"\n",
    "\n",
    "\n",
    "# @markdown **Security Note:** It's best practice to manage API keys securely (e.g., using Colab Secrets or environment variables) rather than hardcoding them directly in the notebook. Replace the placeholder strings above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c29c0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Environment configured.\n"
     ]
    }
   ],
   "source": [
    "# --- Define Model Constants for easier use ---\n",
    "\n",
    "# More supported models can be referenced here: https://ai.google.dev/gemini-api/docs/models#model-variations\n",
    "MODEL_GEMINI_2_0_FLASH = \"gemini-2.0-flash\"\n",
    "\n",
    "# More supported models can be referenced here: https://docs.litellm.ai/docs/providers/openai#openai-chat-completion-models\n",
    "MODEL_GPT_4O = \"openai/gpt-4.1-mini\" # You can also try: gpt-4.1-mini, gpt-4o etc.\n",
    "\n",
    "# More supported models can be referenced here: https://docs.litellm.ai/docs/providers/anthropic\n",
    "MODEL_CLAUDE_SONNET = \"anthropic/claude-haiku-4-5-20251001\" # You can also try: claude-opus-4-20250514 , claude-3-7-sonnet-20250219 etc\n",
    "\n",
    "print(\"\\nEnvironment configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c43e4ca",
   "metadata": {},
   "source": [
    "Key Concept: Docstrings are Crucial! The agent's LLM relies heavily on the function's docstring to understand:\n",
    "\n",
    "    What the tool does.\n",
    "\n",
    "    When to use it.\n",
    "\n",
    "    What arguments it requires (city: str).\n",
    "\n",
    "    What information it returns.\n",
    "\n",
    "Best Practice: Write clear, descriptive, and accurate docstrings for your tools. This is essential for the LLM to use the tool correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb666ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Tool: get_weather called for city: New York ---\n",
      "{'status': 'success', 'report': 'The weather in New York is sunny with a temperature of 25°C.'}\n",
      "--- Tool: get_weather called for city: Paris ---\n",
      "{'status': 'error', 'error_message': \"Sorry, I don't have weather information for 'Paris'.\"}\n"
     ]
    }
   ],
   "source": [
    "# @title Define the get_weather Tool\n",
    "def get_weather(city: str) -> dict:\n",
    "    \"\"\"Retrieves the current weather report for a specified city.\n",
    "\n",
    "    Args:\n",
    "        city (str): The name of the city (e.g., \"New York\", \"London\", \"Tokyo\").\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the weather information.\n",
    "              Includes a 'status' key ('success' or 'error').\n",
    "              If 'success', includes a 'report' key with weather details.\n",
    "              If 'error', includes an 'error_message' key.\n",
    "    \"\"\"\n",
    "    print(f\"--- Tool: get_weather called for city: {city} ---\") # Log tool execution\n",
    "    city_normalized = city.lower().replace(\" \", \"\") # Basic normalization\n",
    "\n",
    "    # Mock weather data\n",
    "    mock_weather_db = {\n",
    "        \"newyork\": {\"status\": \"success\", \"report\": \"The weather in New York is sunny with a temperature of 25°C.\"},\n",
    "        \"london\": {\"status\": \"success\", \"report\": \"It's cloudy in London with a temperature of 15°C.\"},\n",
    "        \"tokyo\": {\"status\": \"success\", \"report\": \"Tokyo is experiencing light rain and a temperature of 18°C.\"},\n",
    "    }\n",
    "\n",
    "    if city_normalized in mock_weather_db:\n",
    "        return mock_weather_db[city_normalized]\n",
    "    else:\n",
    "        return {\"status\": \"error\", \"error_message\": f\"Sorry, I don't have weather information for '{city}'.\"}\n",
    "\n",
    "# Example tool usage (optional test)\n",
    "print(get_weather(\"New York\"))\n",
    "print(get_weather(\"Paris\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5751ad6",
   "metadata": {},
   "source": [
    "Best Practice: Provide clear and specific instruction prompts. The more detailed the instructions, the better the LLM can understand its role and how to use its tools effectively. Be explicit about error handling if needed.\n",
    "\n",
    "Best Practice: Choose descriptive name and description values. These are used internally by ADK and are vital for features like automatic delegation (covered later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3b2d8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 'weather_agent_v1' created using model 'gemini-2.0-flash'.\n"
     ]
    }
   ],
   "source": [
    "# @title Define the Weather Agent\n",
    "# Use one of the model constants defined earlier\n",
    "AGENT_MODEL = MODEL_GEMINI_2_0_FLASH # Starting with Gemini\n",
    "\n",
    "weather_agent = Agent(\n",
    "    name=\"weather_agent_v1\",\n",
    "    model=AGENT_MODEL, # Can be a string for Gemini or a LiteLlm object\n",
    "    description=\"Provides weather information for specific cities.\",\n",
    "    instruction=\"You are a helpful weather assistant. \"\n",
    "                \"When the user asks for the weather in a specific city, \"\n",
    "                \"use the 'get_weather' tool to find the information. \"\n",
    "                \"If the tool returns an error, inform the user politely. \"\n",
    "                \"If the tool is successful, present the weather report clearly.\",\n",
    "    tools=[get_weather], # Pass the function directly\n",
    ")\n",
    "\n",
    "print(f\"Agent '{weather_agent.name}' created using model '{AGENT_MODEL}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bd80d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session created: App='weather_tutorial_app', User='user_1', Session='session_001'\n",
      "Runner created for agent 'weather_agent_v1'.\n"
     ]
    }
   ],
   "source": [
    "# @title Setup Session Service and Runner\n",
    "\n",
    "# --- Session Management ---\n",
    "# Key Concept: SessionService stores conversation history & state.\n",
    "# InMemorySessionService is simple, non-persistent storage for this tutorial.\n",
    "session_service = InMemorySessionService()\n",
    "\n",
    "# Define constants for identifying the interaction context\n",
    "APP_NAME = \"weather_tutorial_app\"\n",
    "USER_ID = \"user_1\"\n",
    "SESSION_ID = \"session_001\" # Using a fixed ID for simplicity\n",
    "\n",
    "# Create the specific session where the conversation will happen\n",
    "session = await session_service.create_session(\n",
    "    app_name=APP_NAME,\n",
    "    user_id=USER_ID,\n",
    "    session_id=SESSION_ID\n",
    ")\n",
    "print(f\"Session created: App='{APP_NAME}', User='{USER_ID}', Session='{SESSION_ID}'\")\n",
    "\n",
    "# --- OR ---\n",
    "\n",
    "# Uncomment the following lines if running as a standard Python script (.py file):\n",
    "\n",
    "# async def init_session(app_name:str,user_id:str,session_id:str) -> InMemorySessionService:\n",
    "#     session = await session_service.create_session(\n",
    "#         app_name=app_name,\n",
    "#         user_id=user_id,\n",
    "#         session_id=session_id\n",
    "#     )\n",
    "#     print(f\"Session created: App='{app_name}', User='{user_id}', Session='{session_id}'\")\n",
    "#     return session\n",
    "# \n",
    "# session = asyncio.run(init_session(APP_NAME,USER_ID,SESSION_ID))\n",
    "\n",
    "# --- Runner ---\n",
    "# Key Concept: Runner orchestrates the agent execution loop.\n",
    "runner = Runner(\n",
    "    agent=weather_agent, # The agent we want to run\n",
    "    app_name=APP_NAME,   # Associates runs with our app\n",
    "    session_service=session_service # Uses our session manager\n",
    ")\n",
    "print(f\"Runner created for agent '{runner.agent.name}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b60feb",
   "metadata": {},
   "source": [
    "So basically we pass the APP Name, user_id, session_id to create a session - basically a graph execution in LangGraph\n",
    "\n",
    "Runner - thi basically creates the graph compile sorts - where everything is set and ready to get invoked. \n",
    "\n",
    "One difference is that ADK's agents are always asynchronous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd28ba3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Define Agent Interaction Function\n",
    "\n",
    "from google.genai import types # For creating message Content/Parts\n",
    "\n",
    "async def call_agent_async(query: str, runner, user_id, session_id):\n",
    "  \"\"\"Sends a query to the agent and prints the final response.\"\"\"\n",
    "  print(f\"\\n>>> User Query: {query}\")\n",
    "\n",
    "  # Prepare the user's message in ADK format\n",
    "  content = types.Content(role='user', parts=[types.Part(text=query)])\n",
    "\n",
    "  final_response_text = \"Agent did not produce a final response.\" # Default\n",
    "\n",
    "  # Key Concept: run_async executes the agent logic and yields Events.\n",
    "  # We iterate through events to find the final answer.\n",
    "  async for event in runner.run_async(user_id=user_id, session_id=session_id, new_message=content):\n",
    "      # You can uncomment the line below to see *all* events during execution\n",
    "      # print(f\"  [Event] Author: {event.author}, Type: {type(event).__name__}, Final: {event.is_final_response()}, Content: {event.content}\")\n",
    "\n",
    "      # Key Concept: is_final_response() marks the concluding message for the turn.\n",
    "      if event.is_final_response():\n",
    "          if event.content and event.content.parts:\n",
    "             # Assuming text response in the first part\n",
    "             final_response_text = event.content.parts[0].text\n",
    "          elif event.actions and event.actions.escalate: # Handle potential errors/escalations\n",
    "             final_response_text = f\"Agent escalated: {event.error_message or 'No specific message.'}\"\n",
    "          # Add more checks here if needed (e.g., specific error codes)\n",
    "          break # Stop processing events once the final response is found\n",
    "\n",
    "  print(f\"<<< Agent Response: {final_response_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aac8b44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> User Query: What is the weather like in London?\n",
      "--- Tool: get_weather called for city: London ---\n",
      "<<< Agent Response: The weather in London is cloudy with a temperature of 15°C.\n",
      "\n",
      "\n",
      ">>> User Query: How about Paris?\n",
      "--- Tool: get_weather called for city: Paris ---\n",
      "<<< Agent Response: Sorry, I don't have weather information for Paris.\n",
      "\n",
      "\n",
      ">>> User Query: Tell me the weather in New York\n",
      "--- Tool: get_weather called for city: New York ---\n",
      "<<< Agent Response: The weather in New York is sunny with a temperature of 25°C.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title Run the Initial Conversation\n",
    "\n",
    "# We need an async function to await our interaction helper\n",
    "async def run_conversation():\n",
    "    await call_agent_async(\"What is the weather like in London?\",\n",
    "                                       runner=runner,\n",
    "                                       user_id=USER_ID,\n",
    "                                       session_id=SESSION_ID)\n",
    "\n",
    "    await call_agent_async(\"How about Paris?\",\n",
    "                                       runner=runner,\n",
    "                                       user_id=USER_ID,\n",
    "                                       session_id=SESSION_ID) # Expecting the tool's error message\n",
    "\n",
    "    await call_agent_async(\"Tell me the weather in New York\",\n",
    "                                       runner=runner,\n",
    "                                       user_id=USER_ID,\n",
    "                                       session_id=SESSION_ID)\n",
    "\n",
    "# Execute the conversation using await in an async context (like Colab/Jupyter)\n",
    "await run_conversation()\n",
    "\n",
    "# --- OR ---\n",
    "\n",
    "# Uncomment the following lines if running as a standard Python script (.py file):\n",
    "# import asyncio\n",
    "# if __name__ == \"__main__\":\n",
    "#     try:\n",
    "#         asyncio.run(run_conversation())\n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7833c8",
   "metadata": {},
   "source": [
    "Basically what you do with agent adk - You create an agent, a tool, a session, a runner to combine agent, tool and session, then you run the runner. Then print the execution. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a9648d",
   "metadata": {},
   "source": [
    "When using LLMs other than google's, we basically invoke the LLM via liteLLM - agent adk has created a wrapper on top of liteLLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f1cbd6",
   "metadata": {},
   "source": [
    "Best Practice: Use constants for model names (like MODEL_GPT_4O, MODEL_CLAUDE_SONNET defined in Step 0) to avoid typos and make code easier to manage.\n",
    "\n",
    "Error Handling: We wrap the agent definitions in try...except blocks. This prevents the entire code cell from failing if an API key for a specific provider is missing or invalid, allowing the tutorial to proceed with the models that are configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "470f5ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 'weather_agent_gpt' created using model 'openai/gpt-4.1-mini'.\n",
      "Session created: App='weather_tutorial_app_gpt', User='user_1_gpt', Session='session_001_gpt'\n",
      "Runner created for agent 'weather_agent_gpt'.\n",
      "\n",
      "--- Testing GPT Agent ---\n",
      "\n",
      ">>> User Query: What's the weather in Tokyo?\n",
      "--- Tool: get_weather called for city: Tokyo ---\n",
      "<<< Agent Response: The weather in Tokyo is currently light rain with a temperature of 18°C.\n"
     ]
    }
   ],
   "source": [
    "# @title Define and Test GPT Agent\n",
    "\n",
    "# Make sure 'get_weather' function from Step 1 is defined in your environment.\n",
    "# Make sure 'call_agent_async' is defined from earlier.\n",
    "\n",
    "# --- Agent using GPT-4o ---\n",
    "weather_agent_gpt = None # Initialize to None\n",
    "runner_gpt = None      # Initialize runner to None\n",
    "\n",
    "try:\n",
    "    weather_agent_gpt = Agent(\n",
    "        name=\"weather_agent_gpt\",\n",
    "        # Key change: Wrap the LiteLLM model identifier\n",
    "        model=LiteLlm(model=MODEL_GPT_4O),\n",
    "        description=\"Provides weather information (using GPT-4o).\",\n",
    "        instruction=\"You are a helpful weather assistant powered by GPT-4o. \"\n",
    "                    \"Use the 'get_weather' tool for city weather requests. \"\n",
    "                    \"Clearly present successful reports or polite error messages based on the tool's output status.\",\n",
    "        tools=[get_weather], # Re-use the same tool\n",
    "    )\n",
    "    print(f\"Agent '{weather_agent_gpt.name}' created using model '{MODEL_GPT_4O}'.\")\n",
    "\n",
    "    # InMemorySessionService is simple, non-persistent storage for this tutorial.\n",
    "    session_service_gpt = InMemorySessionService() # Create a dedicated service\n",
    "\n",
    "    # Define constants for identifying the interaction context\n",
    "    APP_NAME_GPT = \"weather_tutorial_app_gpt\" # Unique app name for this test\n",
    "    USER_ID_GPT = \"user_1_gpt\"\n",
    "    SESSION_ID_GPT = \"session_001_gpt\" # Using a fixed ID for simplicity\n",
    "\n",
    "    # Create the specific session where the conversation will happen\n",
    "    session_gpt = await session_service_gpt.create_session(\n",
    "        app_name=APP_NAME_GPT,\n",
    "        user_id=USER_ID_GPT,\n",
    "        session_id=SESSION_ID_GPT\n",
    "    )\n",
    "    print(f\"Session created: App='{APP_NAME_GPT}', User='{USER_ID_GPT}', Session='{SESSION_ID_GPT}'\")\n",
    "\n",
    "    # Create a runner specific to this agent and its session service\n",
    "    runner_gpt = Runner(\n",
    "        agent=weather_agent_gpt,\n",
    "        app_name=APP_NAME_GPT,       # Use the specific app name\n",
    "        session_service=session_service_gpt # Use the specific session service\n",
    "        )\n",
    "    print(f\"Runner created for agent '{runner_gpt.agent.name}'.\")\n",
    "\n",
    "    # --- Test the GPT Agent ---\n",
    "    print(\"\\n--- Testing GPT Agent ---\")\n",
    "    # Ensure call_agent_async uses the correct runner, user_id, session_id\n",
    "    await call_agent_async(query = \"What's the weather in Tokyo?\",\n",
    "                           runner=runner_gpt,\n",
    "                           user_id=USER_ID_GPT,\n",
    "                           session_id=SESSION_ID_GPT)\n",
    "    # --- OR ---\n",
    "\n",
    "    # Uncomment the following lines if running as a standard Python script (.py file):\n",
    "    # import asyncio\n",
    "    # if __name__ == \"__main__\":\n",
    "    #     try:\n",
    "    #         asyncio.run(call_agent_async(query = \"What's the weather in Tokyo?\",\n",
    "    #                      runner=runner_gpt,\n",
    "    #                       user_id=USER_ID_GPT,\n",
    "    #                       session_id=SESSION_ID_GPT)\n",
    "    #     except Exception as e:\n",
    "    #         print(f\"An error occurred: {e}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Could not create or run GPT agent '{MODEL_GPT_4O}'. Check API Key and model name. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24d6c266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 'weather_agent_claude' created using model 'anthropic/claude-haiku-4-5-20251001'.\n",
      "Session created: App='weather_tutorial_app_claude', User='user_1_claude', Session='session_001_claude'\n",
      "Runner created for agent 'weather_agent_claude'.\n",
      "\n",
      "--- Testing Claude Agent ---\n",
      "\n",
      ">>> User Query: Weather in London please.\n",
      "--- Tool: get_weather called for city: London ---\n",
      "<<< Agent Response: Here's the current weather in London:\n",
      "\n",
      "**London Weather**\n",
      "- **Condition**: Cloudy\n",
      "- **Temperature**: 15°C (59°F)\n",
      "\n",
      "It's a cool, cloudy day in London. You might want to bring an umbrella or a light jacket if you're heading out!\n"
     ]
    }
   ],
   "source": [
    "# @title Define and Test Claude Agent\n",
    "\n",
    "# Make sure 'get_weather' function from Step 1 is defined in your environment.\n",
    "# Make sure 'call_agent_async' is defined from earlier.\n",
    "\n",
    "# --- Agent using Claude Sonnet ---\n",
    "weather_agent_claude = None # Initialize to None\n",
    "runner_claude = None      # Initialize runner to None\n",
    "\n",
    "try:\n",
    "    weather_agent_claude = Agent(\n",
    "        name=\"weather_agent_claude\",\n",
    "        # Key change: Wrap the LiteLLM model identifier\n",
    "        model=LiteLlm(model=MODEL_CLAUDE_SONNET),\n",
    "        description=\"Provides weather information (using Claude Sonnet).\",\n",
    "        instruction=\"You are a helpful weather assistant powered by Claude Sonnet. \"\n",
    "                    \"Use the 'get_weather' tool for city weather requests. \"\n",
    "                    \"Analyze the tool's dictionary output ('status', 'report'/'error_message'). \"\n",
    "                    \"Clearly present successful reports or polite error messages.\",\n",
    "        tools=[get_weather], # Re-use the same tool\n",
    "    )\n",
    "    print(f\"Agent '{weather_agent_claude.name}' created using model '{MODEL_CLAUDE_SONNET}'.\")\n",
    "\n",
    "    # InMemorySessionService is simple, non-persistent storage for this tutorial.\n",
    "    session_service_claude = InMemorySessionService() # Create a dedicated service\n",
    "\n",
    "    # Define constants for identifying the interaction context\n",
    "    APP_NAME_CLAUDE = \"weather_tutorial_app_claude\" # Unique app name\n",
    "    USER_ID_CLAUDE = \"user_1_claude\"\n",
    "    SESSION_ID_CLAUDE = \"session_001_claude\" # Using a fixed ID for simplicity\n",
    "\n",
    "    # Create the specific session where the conversation will happen\n",
    "    session_claude = await session_service_claude.create_session(\n",
    "        app_name=APP_NAME_CLAUDE,\n",
    "        user_id=USER_ID_CLAUDE,\n",
    "        session_id=SESSION_ID_CLAUDE\n",
    "    )\n",
    "    print(f\"Session created: App='{APP_NAME_CLAUDE}', User='{USER_ID_CLAUDE}', Session='{SESSION_ID_CLAUDE}'\")\n",
    "\n",
    "    # Create a runner specific to this agent and its session service\n",
    "    runner_claude = Runner(\n",
    "        agent=weather_agent_claude,\n",
    "        app_name=APP_NAME_CLAUDE,       # Use the specific app name\n",
    "        session_service=session_service_claude # Use the specific session service\n",
    "        )\n",
    "    print(f\"Runner created for agent '{runner_claude.agent.name}'.\")\n",
    "\n",
    "    # --- Test the Claude Agent ---\n",
    "    print(\"\\n--- Testing Claude Agent ---\")\n",
    "    # Ensure call_agent_async uses the correct runner, user_id, session_id\n",
    "    await call_agent_async(query = \"Weather in London please.\",\n",
    "                           runner=runner_claude,\n",
    "                           user_id=USER_ID_CLAUDE,\n",
    "                           session_id=SESSION_ID_CLAUDE)\n",
    "\n",
    "    # --- OR ---\n",
    "\n",
    "    # Uncomment the following lines if running as a standard Python script (.py file):\n",
    "    # import asyncio\n",
    "    # if __name__ == \"__main__\":\n",
    "    #     try:\n",
    "    #         asyncio.run(call_agent_async(query = \"Weather in London please.\",\n",
    "    #                      runner=runner_claude,\n",
    "    #                       user_id=USER_ID_CLAUDE,\n",
    "    #                       session_id=SESSION_ID_CLAUDE)\n",
    "    #     except Exception as e:\n",
    "    #         print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Could not create or run Claude agent '{MODEL_CLAUDE_SONNET}'. Check API Key and model name. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f41658",
   "metadata": {},
   "source": [
    "We built and experimented with a single agent focused solely on weather lookups. While effective for its specific task, real-world applications often involve handling a wider variety of user interactions. We could keep adding more tools and complex instructions to our single weather agent, but this can quickly become unmanageable and less efficient.\n",
    "\n",
    "A more robust approach is to build an Agent Team. This involves:\n",
    "\n",
    "    Creating multiple, specialized agents, each designed for a specific capability (e.g., one for weather, one for greetings, one for calculations).\n",
    "\n",
    "    Designating a root agent (or orchestrator) that receives the initial user request.\n",
    "\n",
    "    Enabling the root agent to delegate the request to the most appropriate specialized sub-agent based on the user's intent.\n",
    "\n",
    "\n",
    "This is what we exactly did in Mentornaut"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef15f3ed",
   "metadata": {},
   "source": [
    "## Building an Agent Team - Delegation for Greetings & Farewells¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffafa0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greeting and Farewell tools defined.\n",
      "--- Tool: say_hello called with name: Alice ---\n",
      "Hello, Alice!\n",
      "--- Tool: say_hello called without a specific name (name_arg_value: None) ---\n",
      "Hello there!\n",
      "--- Tool: say_hello called without a specific name (name_arg_value: None) ---\n",
      "Hello there!\n"
     ]
    }
   ],
   "source": [
    "# @title Define Tools for Greeting and Farewell Agents\n",
    "from typing import Optional # Make sure to import Optional\n",
    "\n",
    "# Ensure 'get_weather' from Step 1 is available if running this step independently.\n",
    "# def get_weather(city: str) -> dict: ... (from Step 1)\n",
    "\n",
    "def say_hello(name: Optional[str] = None) -> str:\n",
    "    \"\"\"Provides a simple greeting. If a name is provided, it will be used.\n",
    "\n",
    "    Args:\n",
    "        name (str, optional): The name of the person to greet. Defaults to a generic greeting if not provided.\n",
    "\n",
    "    Returns:\n",
    "        str: A friendly greeting message.\n",
    "    \"\"\"\n",
    "    if name:\n",
    "        greeting = f\"Hello, {name}!\"\n",
    "        print(f\"--- Tool: say_hello called with name: {name} ---\")\n",
    "    else:\n",
    "        greeting = \"Hello there!\" # Default greeting if name is None or not explicitly passed\n",
    "        print(f\"--- Tool: say_hello called without a specific name (name_arg_value: {name}) ---\")\n",
    "    return greeting\n",
    "\n",
    "def say_goodbye() -> str:\n",
    "    \"\"\"Provides a simple farewell message to conclude the conversation.\"\"\"\n",
    "    print(f\"--- Tool: say_goodbye called ---\")\n",
    "    return \"Goodbye! Have a great day.\"\n",
    "\n",
    "print(\"Greeting and Farewell tools defined.\")\n",
    "\n",
    "# Optional self-test\n",
    "print(say_hello(\"Alice\"))\n",
    "print(say_hello()) # Test with no argument (should use default \"Hello there!\")\n",
    "print(say_hello(name=None)) # Test with name explicitly as None (should use default \"Hello there!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee4dcf9",
   "metadata": {},
   "source": [
    "Notice their highly focused instruction and, critically, their clear description. The description is the primary information the root agent uses to decide when to delegate to these sub-agents.\n",
    "\n",
    "Best Practice: Sub-agent description fields should accurately and concisely summarize their specific capability. This is crucial for effective automatic delegation.\n",
    "\n",
    "Best Practice: Sub-agent instruction fields should be tailored to their limited scope, telling them exactly what to do and what not to do (e.g., \"Your only task is...\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06e7fc27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Agent 'greeting_agent' created using model 'gemini-2.0-flash'.\n",
      "✅ Agent 'farewell_agent' created using model 'gemini-2.0-flash'.\n"
     ]
    }
   ],
   "source": [
    "# @title Define Greeting and Farewell Sub-Agents\n",
    "\n",
    "# If you want to use models other than Gemini, Ensure LiteLlm is imported and API keys are set (from Step 0/2)\n",
    "# from google.adk.models.lite_llm import LiteLlm\n",
    "# MODEL_GPT_4O, MODEL_CLAUDE_SONNET etc. should be defined\n",
    "# Or else, continue to use: model = MODEL_GEMINI_2_0_FLASH\n",
    "\n",
    "# --- Greeting Agent ---\n",
    "greeting_agent = None\n",
    "try:\n",
    "    greeting_agent = Agent(\n",
    "        # Using a potentially different/cheaper model for a simple task\n",
    "        model = MODEL_GEMINI_2_0_FLASH,\n",
    "        # model=LiteLlm(model=MODEL_GPT_4O), # If you would like to experiment with other models\n",
    "        name=\"greeting_agent\",\n",
    "        instruction=\"You are the Greeting Agent. Your ONLY task is to provide a friendly greeting to the user. \"\n",
    "                    \"Use the 'say_hello' tool to generate the greeting. \"\n",
    "                    \"If the user provides their name, make sure to pass it to the tool. \"\n",
    "                    \"Do not engage in any other conversation or tasks.\",\n",
    "        description=\"Handles simple greetings and hellos using the 'say_hello' tool.\", # Crucial for delegation\n",
    "        tools=[say_hello],\n",
    "    )\n",
    "    print(f\"✅ Agent '{greeting_agent.name}' created using model '{greeting_agent.model}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Could not create Greeting agent. Check API Key ({greeting_agent.model}). Error: {e}\")\n",
    "\n",
    "# --- Farewell Agent ---\n",
    "farewell_agent = None\n",
    "try:\n",
    "    farewell_agent = Agent(\n",
    "        # Can use the same or a different model\n",
    "        model = MODEL_GEMINI_2_0_FLASH,\n",
    "        # model=LiteLlm(model=MODEL_GPT_4O), # If you would like to experiment with other models\n",
    "        name=\"farewell_agent\",\n",
    "        instruction=\"You are the Farewell Agent. Your ONLY task is to provide a polite goodbye message. \"\n",
    "                    \"Use the 'say_goodbye' tool when the user indicates they are leaving or ending the conversation \"\n",
    "                    \"(e.g., using words like 'bye', 'goodbye', 'thanks bye', 'see you'). \"\n",
    "                    \"Do not perform any other actions.\",\n",
    "        description=\"Handles simple farewells and goodbyes using the 'say_goodbye' tool.\", # Crucial for delegation\n",
    "        tools=[say_goodbye],\n",
    "    )\n",
    "    print(f\"✅ Agent '{farewell_agent.name}' created using model '{farewell_agent.model}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Could not create Farewell agent. Check API Key ({farewell_agent.model}). Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19736c73",
   "metadata": {},
   "source": [
    "Key Concept: Automatic Delegation (Auto Flow) By providing the sub_agents list, ADK enables automatic delegation. When the root agent receives a user query, its LLM considers not only its own instructions and tools but also the description of each sub-agent. If the LLM determines that a query aligns better with a sub-agent's described capability (e.g., \"Handles simple greetings\"), it will automatically generate a special internal action to transfer control to that sub-agent for that turn. The sub-agent then processes the query using its own model, instructions, and tools."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96232b1a",
   "metadata": {},
   "source": [
    "Best Practice: Ensure the root agent's instructions clearly guide its delegation decisions. Mention the sub-agents by name and describe the conditions under which delegation should occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fc03e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Root Agent 'weather_agent_v2' created using model 'gemini-2.0-flash' with sub-agents: ['greeting_agent', 'farewell_agent']\n"
     ]
    }
   ],
   "source": [
    "# @title Define the Root Agent with Sub-Agents\n",
    "\n",
    "# Ensure sub-agents were created successfully before defining the root agent.\n",
    "# Also ensure the original 'get_weather' tool is defined.\n",
    "root_agent = None\n",
    "runner_root = None # Initialize runner\n",
    "\n",
    "if greeting_agent and farewell_agent and 'get_weather' in globals():\n",
    "    # Let's use a capable Gemini model for the root agent to handle orchestration\n",
    "    root_agent_model = MODEL_GEMINI_2_0_FLASH\n",
    "\n",
    "    weather_agent_team = Agent(\n",
    "        name=\"weather_agent_v2\", # Give it a new version name\n",
    "        model=root_agent_model,\n",
    "        description=\"The main coordinator agent. Handles weather requests and delegates greetings/farewells to specialists.\",\n",
    "        instruction=\"You are the main Weather Agent coordinating a team. Your primary responsibility is to provide weather information. \"\n",
    "                    \"Use the 'get_weather' tool ONLY for specific weather requests (e.g., 'weather in London'). \"\n",
    "                    \"You have specialized sub-agents: \"\n",
    "                    \"1. 'greeting_agent': Handles simple greetings like 'Hi', 'Hello'. Delegate to it for these. \"\n",
    "                    \"2. 'farewell_agent': Handles simple farewells like 'Bye', 'See you'. Delegate to it for these. \"\n",
    "                    \"Analyze the user's query. If it's a greeting, delegate to 'greeting_agent'. If it's a farewell, delegate to 'farewell_agent'. \"\n",
    "                    \"If it's a weather request, handle it yourself using 'get_weather'. \"\n",
    "                    \"For anything else, respond appropriately or state you cannot handle it.\",\n",
    "        tools=[get_weather], # Root agent still needs the weather tool for its core task\n",
    "        # Key change: Link the sub-agents here!\n",
    "        sub_agents=[greeting_agent, farewell_agent]\n",
    "    )\n",
    "    print(f\"✅ Root Agent '{weather_agent_team.name}' created using model '{root_agent_model}' with sub-agents: {[sa.name for sa in weather_agent_team.sub_agents]}\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ Cannot create root agent because one or more sub-agents failed to initialize or 'get_weather' tool is missing.\")\n",
    "    if not greeting_agent: print(\" - Greeting Agent is missing.\")\n",
    "    if not farewell_agent: print(\" - Farewell Agent is missing.\")\n",
    "    if 'get_weather' not in globals(): print(\" - get_weather function is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7234abf4",
   "metadata": {},
   "source": [
    "Should understand how the subagents descriptions are passed into the manager agent - should check the source code in python adk github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57b07103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__name__': '__main__',\n",
       " '__doc__': 'Automatically created module for IPython interactive environment',\n",
       " '__package__': None,\n",
       " '__loader__': None,\n",
       " '__spec__': None,\n",
       " '__builtin__': <module 'builtins' (built-in)>,\n",
       " '__builtins__': <module 'builtins' (built-in)>,\n",
       " '_ih': ['',\n",
       "  '# @title Import necessary libraries\\nimport os\\nimport asyncio\\nfrom google.adk.agents import Agent\\nfrom google.adk.models.lite_llm import LiteLlm # For multi-model support\\nfrom google.adk.sessions import InMemorySessionService\\nfrom google.adk.runners import Runner\\nfrom google.genai import types # For creating message Content/Parts\\n\\nimport warnings\\n# Ignore all warnings\\nwarnings.filterwarnings(\"ignore\")\\n\\nimport logging\\nlogging.basicConfig(level=logging.ERROR)\\n\\nprint(\"Libraries imported.\")',\n",
       "  'from dotenv import load_dotenv\\nload_dotenv()',\n",
       "  '# @title Configure API Keys (Replace with your actual keys!)\\n\\n# --- Verify Keys (Optional Check) ---\\nprint(\"API Keys Set:\")\\nprint(f\"Google API Key set: {\\'Yes\\' if os.environ.get(\\'GOOGLE_API_KEY\\') and os.environ[\\'GOOGLE_API_KEY\\'] != \\'YOUR_GOOGLE_API_KEY\\' else \\'No (REPLACE PLACEHOLDER!)\\'}\")\\nprint(f\"OpenAI API Key set: {\\'Yes\\' if os.environ.get(\\'OPENAI_API_KEY\\') and os.environ[\\'OPENAI_API_KEY\\'] != \\'YOUR_OPENAI_API_KEY\\' else \\'No (REPLACE PLACEHOLDER!)\\'}\")\\nprint(f\"Anthropic API Key set: {\\'Yes\\' if os.environ.get(\\'ANTHROPIC_API_KEY\\') and os.environ[\\'ANTHROPIC_API_KEY\\'] != \\'YOUR_ANTHROPIC_API_KEY\\' else \\'No (REPLACE PLACEHOLDER!)\\'}\")\\n\\n# Configure ADK to use API keys directly (not Vertex AI for this multi-model setup)\\nos.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = \"False\"\\n\\n\\n# @markdown **Security Note:** It\\'s best practice to manage API keys securely (e.g., using Colab Secrets or environment variables) rather than hardcoding them directly in the notebook. Replace the placeholder strings above.',\n",
       "  '# --- Define Model Constants for easier use ---\\n\\n# More supported models can be referenced here: https://ai.google.dev/gemini-api/docs/models#model-variations\\nMODEL_GEMINI_2_0_FLASH = \"gemini-2.0-flash\"\\n\\n# More supported models can be referenced here: https://docs.litellm.ai/docs/providers/openai#openai-chat-completion-models\\nMODEL_GPT_4O = \"openai/gpt-4.1-mini\" # You can also try: gpt-4.1-mini, gpt-4o etc.\\n\\n# More supported models can be referenced here: https://docs.litellm.ai/docs/providers/anthropic\\nMODEL_CLAUDE_SONNET = \"anthropic/claude-haiku-4-5-20251001\" # You can also try: claude-opus-4-20250514 , claude-3-7-sonnet-20250219 etc\\n\\nprint(\"\\\\nEnvironment configured.\")',\n",
       "  '# @title Define the get_weather Tool\\ndef get_weather(city: str) -> dict:\\n    \"\"\"Retrieves the current weather report for a specified city.\\n\\n    Args:\\n        city (str): The name of the city (e.g., \"New York\", \"London\", \"Tokyo\").\\n\\n    Returns:\\n        dict: A dictionary containing the weather information.\\n              Includes a \\'status\\' key (\\'success\\' or \\'error\\').\\n              If \\'success\\', includes a \\'report\\' key with weather details.\\n              If \\'error\\', includes an \\'error_message\\' key.\\n    \"\"\"\\n    print(f\"--- Tool: get_weather called for city: {city} ---\") # Log tool execution\\n    city_normalized = city.lower().replace(\" \", \"\") # Basic normalization\\n\\n    # Mock weather data\\n    mock_weather_db = {\\n        \"newyork\": {\"status\": \"success\", \"report\": \"The weather in New York is sunny with a temperature of 25°C.\"},\\n        \"london\": {\"status\": \"success\", \"report\": \"It\\'s cloudy in London with a temperature of 15°C.\"},\\n        \"tokyo\": {\"status\": \"success\", \"report\": \"Tokyo is experiencing light rain and a temperature of 18°C.\"},\\n    }\\n\\n    if city_normalized in mock_weather_db:\\n        return mock_weather_db[city_normalized]\\n    else:\\n        return {\"status\": \"error\", \"error_message\": f\"Sorry, I don\\'t have weather information for \\'{city}\\'.\"}\\n\\n# Example tool usage (optional test)\\nprint(get_weather(\"New York\"))\\nprint(get_weather(\"Paris\"))',\n",
       "  '# @title Define the Weather Agent\\n# Use one of the model constants defined earlier\\nAGENT_MODEL = MODEL_GEMINI_2_0_FLASH # Starting with Gemini\\n\\nweather_agent = Agent(\\n    name=\"weather_agent_v1\",\\n    model=AGENT_MODEL, # Can be a string for Gemini or a LiteLlm object\\n    description=\"Provides weather information for specific cities.\",\\n    instruction=\"You are a helpful weather assistant. \"\\n                \"When the user asks for the weather in a specific city, \"\\n                \"use the \\'get_weather\\' tool to find the information. \"\\n                \"If the tool returns an error, inform the user politely. \"\\n                \"If the tool is successful, present the weather report clearly.\",\\n    tools=[get_weather], # Pass the function directly\\n)\\n\\nprint(f\"Agent \\'{weather_agent.name}\\' created using model \\'{AGENT_MODEL}\\'.\")',\n",
       "  '# @title Setup Session Service and Runner\\n\\n# --- Session Management ---\\n# Key Concept: SessionService stores conversation history & state.\\n# InMemorySessionService is simple, non-persistent storage for this tutorial.\\nsession_service = InMemorySessionService()\\n\\n# Define constants for identifying the interaction context\\nAPP_NAME = \"weather_tutorial_app\"\\nUSER_ID = \"user_1\"\\nSESSION_ID = \"session_001\" # Using a fixed ID for simplicity\\n\\n# Create the specific session where the conversation will happen\\nsession = await session_service.create_session(\\n    app_name=APP_NAME,\\n    user_id=USER_ID,\\n    session_id=SESSION_ID\\n)\\nprint(f\"Session created: App=\\'{APP_NAME}\\', User=\\'{USER_ID}\\', Session=\\'{SESSION_ID}\\'\")\\n\\n# --- OR ---\\n\\n# Uncomment the following lines if running as a standard Python script (.py file):\\n\\n# async def init_session(app_name:str,user_id:str,session_id:str) -> InMemorySessionService:\\n#     session = await session_service.create_session(\\n#         app_name=app_name,\\n#         user_id=user_id,\\n#         session_id=session_id\\n#     )\\n#     print(f\"Session created: App=\\'{app_name}\\', User=\\'{user_id}\\', Session=\\'{session_id}\\'\")\\n#     return session\\n# \\n# session = asyncio.run(init_session(APP_NAME,USER_ID,SESSION_ID))\\n\\n# --- Runner ---\\n# Key Concept: Runner orchestrates the agent execution loop.\\nrunner = Runner(\\n    agent=weather_agent, # The agent we want to run\\n    app_name=APP_NAME,   # Associates runs with our app\\n    session_service=session_service # Uses our session manager\\n)\\nprint(f\"Runner created for agent \\'{runner.agent.name}\\'.\")',\n",
       "  '# @title Define Agent Interaction Function\\n\\nfrom google.genai import types # For creating message Content/Parts\\n\\nasync def call_agent_async(query: str, runner, user_id, session_id):\\n  \"\"\"Sends a query to the agent and prints the final response.\"\"\"\\n  print(f\"\\\\n>>> User Query: {query}\")\\n\\n  # Prepare the user\\'s message in ADK format\\n  content = types.Content(role=\\'user\\', parts=[types.Part(text=query)])\\n\\n  final_response_text = \"Agent did not produce a final response.\" # Default\\n\\n  # Key Concept: run_async executes the agent logic and yields Events.\\n  # We iterate through events to find the final answer.\\n  async for event in runner.run_async(user_id=user_id, session_id=session_id, new_message=content):\\n      # You can uncomment the line below to see *all* events during execution\\n      # print(f\"  [Event] Author: {event.author}, Type: {type(event).__name__}, Final: {event.is_final_response()}, Content: {event.content}\")\\n\\n      # Key Concept: is_final_response() marks the concluding message for the turn.\\n      if event.is_final_response():\\n          if event.content and event.content.parts:\\n             # Assuming text response in the first part\\n             final_response_text = event.content.parts[0].text\\n          elif event.actions and event.actions.escalate: # Handle potential errors/escalations\\n             final_response_text = f\"Agent escalated: {event.error_message or \\'No specific message.\\'}\"\\n          # Add more checks here if needed (e.g., specific error codes)\\n          break # Stop processing events once the final response is found\\n\\n  print(f\"<<< Agent Response: {final_response_text}\")',\n",
       "  '# @title Run the Initial Conversation\\n\\n# We need an async function to await our interaction helper\\nasync def run_conversation():\\n    await call_agent_async(\"What is the weather like in London?\",\\n                                       runner=runner,\\n                                       user_id=USER_ID,\\n                                       session_id=SESSION_ID)\\n\\n    await call_agent_async(\"How about Paris?\",\\n                                       runner=runner,\\n                                       user_id=USER_ID,\\n                                       session_id=SESSION_ID) # Expecting the tool\\'s error message\\n\\n    await call_agent_async(\"Tell me the weather in New York\",\\n                                       runner=runner,\\n                                       user_id=USER_ID,\\n                                       session_id=SESSION_ID)\\n\\n# Execute the conversation using await in an async context (like Colab/Jupyter)\\nawait run_conversation()\\n\\n# --- OR ---\\n\\n# Uncomment the following lines if running as a standard Python script (.py file):\\n# import asyncio\\n# if __name__ == \"__main__\":\\n#     try:\\n#         asyncio.run(run_conversation())\\n#     except Exception as e:\\n#         print(f\"An error occurred: {e}\")',\n",
       "  '# @title Define and Test GPT Agent\\n\\n# Make sure \\'get_weather\\' function from Step 1 is defined in your environment.\\n# Make sure \\'call_agent_async\\' is defined from earlier.\\n\\n# --- Agent using GPT-4o ---\\nweather_agent_gpt = None # Initialize to None\\nrunner_gpt = None      # Initialize runner to None\\n\\ntry:\\n    weather_agent_gpt = Agent(\\n        name=\"weather_agent_gpt\",\\n        # Key change: Wrap the LiteLLM model identifier\\n        model=LiteLlm(model=MODEL_GPT_4O),\\n        description=\"Provides weather information (using GPT-4o).\",\\n        instruction=\"You are a helpful weather assistant powered by GPT-4o. \"\\n                    \"Use the \\'get_weather\\' tool for city weather requests. \"\\n                    \"Clearly present successful reports or polite error messages based on the tool\\'s output status.\",\\n        tools=[get_weather], # Re-use the same tool\\n    )\\n    print(f\"Agent \\'{weather_agent_gpt.name}\\' created using model \\'{MODEL_GPT_4O}\\'.\")\\n\\n    # InMemorySessionService is simple, non-persistent storage for this tutorial.\\n    session_service_gpt = InMemorySessionService() # Create a dedicated service\\n\\n    # Define constants for identifying the interaction context\\n    APP_NAME_GPT = \"weather_tutorial_app_gpt\" # Unique app name for this test\\n    USER_ID_GPT = \"user_1_gpt\"\\n    SESSION_ID_GPT = \"session_001_gpt\" # Using a fixed ID for simplicity\\n\\n    # Create the specific session where the conversation will happen\\n    session_gpt = await session_service_gpt.create_session(\\n        app_name=APP_NAME_GPT,\\n        user_id=USER_ID_GPT,\\n        session_id=SESSION_ID_GPT\\n    )\\n    print(f\"Session created: App=\\'{APP_NAME_GPT}\\', User=\\'{USER_ID_GPT}\\', Session=\\'{SESSION_ID_GPT}\\'\")\\n\\n    # Create a runner specific to this agent and its session service\\n    runner_gpt = Runner(\\n        agent=weather_agent_gpt,\\n        app_name=APP_NAME_GPT,       # Use the specific app name\\n        session_service=session_service_gpt # Use the specific session service\\n        )\\n    print(f\"Runner created for agent \\'{runner_gpt.agent.name}\\'.\")\\n\\n    # --- Test the GPT Agent ---\\n    print(\"\\\\n--- Testing GPT Agent ---\")\\n    # Ensure call_agent_async uses the correct runner, user_id, session_id\\n    await call_agent_async(query = \"What\\'s the weather in Tokyo?\",\\n                           runner=runner_gpt,\\n                           user_id=USER_ID_GPT,\\n                           session_id=SESSION_ID_GPT)\\n    # --- OR ---\\n\\n    # Uncomment the following lines if running as a standard Python script (.py file):\\n    # import asyncio\\n    # if __name__ == \"__main__\":\\n    #     try:\\n    #         asyncio.run(call_agent_async(query = \"What\\'s the weather in Tokyo?\",\\n    #                      runner=runner_gpt,\\n    #                       user_id=USER_ID_GPT,\\n    #                       session_id=SESSION_ID_GPT)\\n    #     except Exception as e:\\n    #         print(f\"An error occurred: {e}\")\\n\\nexcept Exception as e:\\n    print(f\"❌ Could not create or run GPT agent \\'{MODEL_GPT_4O}\\'. Check API Key and model name. Error: {e}\")',\n",
       "  '# @title Define and Test Claude Agent\\n\\n# Make sure \\'get_weather\\' function from Step 1 is defined in your environment.\\n# Make sure \\'call_agent_async\\' is defined from earlier.\\n\\n# --- Agent using Claude Sonnet ---\\nweather_agent_claude = None # Initialize to None\\nrunner_claude = None      # Initialize runner to None\\n\\ntry:\\n    weather_agent_claude = Agent(\\n        name=\"weather_agent_claude\",\\n        # Key change: Wrap the LiteLLM model identifier\\n        model=LiteLlm(model=MODEL_CLAUDE_SONNET),\\n        description=\"Provides weather information (using Claude Sonnet).\",\\n        instruction=\"You are a helpful weather assistant powered by Claude Sonnet. \"\\n                    \"Use the \\'get_weather\\' tool for city weather requests. \"\\n                    \"Analyze the tool\\'s dictionary output (\\'status\\', \\'report\\'/\\'error_message\\'). \"\\n                    \"Clearly present successful reports or polite error messages.\",\\n        tools=[get_weather], # Re-use the same tool\\n    )\\n    print(f\"Agent \\'{weather_agent_claude.name}\\' created using model \\'{MODEL_CLAUDE_SONNET}\\'.\")\\n\\n    # InMemorySessionService is simple, non-persistent storage for this tutorial.\\n    session_service_claude = InMemorySessionService() # Create a dedicated service\\n\\n    # Define constants for identifying the interaction context\\n    APP_NAME_CLAUDE = \"weather_tutorial_app_claude\" # Unique app name\\n    USER_ID_CLAUDE = \"user_1_claude\"\\n    SESSION_ID_CLAUDE = \"session_001_claude\" # Using a fixed ID for simplicity\\n\\n    # Create the specific session where the conversation will happen\\n    session_claude = await session_service_claude.create_session(\\n        app_name=APP_NAME_CLAUDE,\\n        user_id=USER_ID_CLAUDE,\\n        session_id=SESSION_ID_CLAUDE\\n    )\\n    print(f\"Session created: App=\\'{APP_NAME_CLAUDE}\\', User=\\'{USER_ID_CLAUDE}\\', Session=\\'{SESSION_ID_CLAUDE}\\'\")\\n\\n    # Create a runner specific to this agent and its session service\\n    runner_claude = Runner(\\n        agent=weather_agent_claude,\\n        app_name=APP_NAME_CLAUDE,       # Use the specific app name\\n        session_service=session_service_claude # Use the specific session service\\n        )\\n    print(f\"Runner created for agent \\'{runner_claude.agent.name}\\'.\")\\n\\n    # --- Test the Claude Agent ---\\n    print(\"\\\\n--- Testing Claude Agent ---\")\\n    # Ensure call_agent_async uses the correct runner, user_id, session_id\\n    await call_agent_async(query = \"Weather in London please.\",\\n                           runner=runner_claude,\\n                           user_id=USER_ID_CLAUDE,\\n                           session_id=SESSION_ID_CLAUDE)\\n\\n    # --- OR ---\\n\\n    # Uncomment the following lines if running as a standard Python script (.py file):\\n    # import asyncio\\n    # if __name__ == \"__main__\":\\n    #     try:\\n    #         asyncio.run(call_agent_async(query = \"Weather in London please.\",\\n    #                      runner=runner_claude,\\n    #                       user_id=USER_ID_CLAUDE,\\n    #                       session_id=SESSION_ID_CLAUDE)\\n    #     except Exception as e:\\n    #         print(f\"An error occurred: {e}\")\\n\\n\\nexcept Exception as e:\\n    print(f\"❌ Could not create or run Claude agent \\'{MODEL_CLAUDE_SONNET}\\'. Check API Key and model name. Error: {e}\")',\n",
       "  '# @title Define Tools for Greeting and Farewell Agents\\nfrom typing import Optional # Make sure to import Optional\\n\\n# Ensure \\'get_weather\\' from Step 1 is available if running this step independently.\\n# def get_weather(city: str) -> dict: ... (from Step 1)\\n\\ndef say_hello(name: Optional[str] = None) -> str:\\n    \"\"\"Provides a simple greeting. If a name is provided, it will be used.\\n\\n    Args:\\n        name (str, optional): The name of the person to greet. Defaults to a generic greeting if not provided.\\n\\n    Returns:\\n        str: A friendly greeting message.\\n    \"\"\"\\n    if name:\\n        greeting = f\"Hello, {name}!\"\\n        print(f\"--- Tool: say_hello called with name: {name} ---\")\\n    else:\\n        greeting = \"Hello there!\" # Default greeting if name is None or not explicitly passed\\n        print(f\"--- Tool: say_hello called without a specific name (name_arg_value: {name}) ---\")\\n    return greeting\\n\\ndef say_goodbye() -> str:\\n    \"\"\"Provides a simple farewell message to conclude the conversation.\"\"\"\\n    print(f\"--- Tool: say_goodbye called ---\")\\n    return \"Goodbye! Have a great day.\"\\n\\nprint(\"Greeting and Farewell tools defined.\")\\n\\n# Optional self-test\\nprint(say_hello(\"Alice\"))\\nprint(say_hello()) # Test with no argument (should use default \"Hello there!\")\\nprint(say_hello(name=None)) # Test with name explicitly as None (should use default \"Hello there!\")',\n",
       "  '# @title Define Greeting and Farewell Sub-Agents\\n\\n# If you want to use models other than Gemini, Ensure LiteLlm is imported and API keys are set (from Step 0/2)\\n# from google.adk.models.lite_llm import LiteLlm\\n# MODEL_GPT_4O, MODEL_CLAUDE_SONNET etc. should be defined\\n# Or else, continue to use: model = MODEL_GEMINI_2_0_FLASH\\n\\n# --- Greeting Agent ---\\ngreeting_agent = None\\ntry:\\n    greeting_agent = Agent(\\n        # Using a potentially different/cheaper model for a simple task\\n        model = MODEL_GEMINI_2_0_FLASH,\\n        # model=LiteLlm(model=MODEL_GPT_4O), # If you would like to experiment with other models\\n        name=\"greeting_agent\",\\n        instruction=\"You are the Greeting Agent. Your ONLY task is to provide a friendly greeting to the user. \"\\n                    \"Use the \\'say_hello\\' tool to generate the greeting. \"\\n                    \"If the user provides their name, make sure to pass it to the tool. \"\\n                    \"Do not engage in any other conversation or tasks.\",\\n        description=\"Handles simple greetings and hellos using the \\'say_hello\\' tool.\", # Crucial for delegation\\n        tools=[say_hello],\\n    )\\n    print(f\"✅ Agent \\'{greeting_agent.name}\\' created using model \\'{greeting_agent.model}\\'.\")\\nexcept Exception as e:\\n    print(f\"❌ Could not create Greeting agent. Check API Key ({greeting_agent.model}). Error: {e}\")\\n\\n# --- Farewell Agent ---\\nfarewell_agent = None\\ntry:\\n    farewell_agent = Agent(\\n        # Can use the same or a different model\\n        model = MODEL_GEMINI_2_0_FLASH,\\n        # model=LiteLlm(model=MODEL_GPT_4O), # If you would like to experiment with other models\\n        name=\"farewell_agent\",\\n        instruction=\"You are the Farewell Agent. Your ONLY task is to provide a polite goodbye message. \"\\n                    \"Use the \\'say_goodbye\\' tool when the user indicates they are leaving or ending the conversation \"\\n                    \"(e.g., using words like \\'bye\\', \\'goodbye\\', \\'thanks bye\\', \\'see you\\'). \"\\n                    \"Do not perform any other actions.\",\\n        description=\"Handles simple farewells and goodbyes using the \\'say_goodbye\\' tool.\", # Crucial for delegation\\n        tools=[say_goodbye],\\n    )\\n    print(f\"✅ Agent \\'{farewell_agent.name}\\' created using model \\'{farewell_agent.model}\\'.\")\\nexcept Exception as e:\\n    print(f\"❌ Could not create Farewell agent. Check API Key ({farewell_agent.model}). Error: {e}\")',\n",
       "  '# @title Define the Root Agent with Sub-Agents\\n\\n# Ensure sub-agents were created successfully before defining the root agent.\\n# Also ensure the original \\'get_weather\\' tool is defined.\\nroot_agent = None\\nrunner_root = None # Initialize runner\\n\\nif greeting_agent and farewell_agent and \\'get_weather\\' in globals():\\n    # Let\\'s use a capable Gemini model for the root agent to handle orchestration\\n    root_agent_model = MODEL_GEMINI_2_0_FLASH\\n\\n    weather_agent_team = Agent(\\n        name=\"weather_agent_v2\", # Give it a new version name\\n        model=root_agent_model,\\n        description=\"The main coordinator agent. Handles weather requests and delegates greetings/farewells to specialists.\",\\n        instruction=\"You are the main Weather Agent coordinating a team. Your primary responsibility is to provide weather information. \"\\n                    \"Use the \\'get_weather\\' tool ONLY for specific weather requests (e.g., \\'weather in London\\'). \"\\n                    \"You have specialized sub-agents: \"\\n                    \"1. \\'greeting_agent\\': Handles simple greetings like \\'Hi\\', \\'Hello\\'. Delegate to it for these. \"\\n                    \"2. \\'farewell_agent\\': Handles simple farewells like \\'Bye\\', \\'See you\\'. Delegate to it for these. \"\\n                    \"Analyze the user\\'s query. If it\\'s a greeting, delegate to \\'greeting_agent\\'. If it\\'s a farewell, delegate to \\'farewell_agent\\'. \"\\n                    \"If it\\'s a weather request, handle it yourself using \\'get_weather\\'. \"\\n                    \"For anything else, respond appropriately or state you cannot handle it.\",\\n        tools=[get_weather], # Root agent still needs the weather tool for its core task\\n        # Key change: Link the sub-agents here!\\n        sub_agents=[greeting_agent, farewell_agent]\\n    )\\n    print(f\"✅ Root Agent \\'{weather_agent_team.name}\\' created using model \\'{root_agent_model}\\' with sub-agents: {[sa.name for sa in weather_agent_team.sub_agents]}\")\\n\\nelse:\\n    print(\"❌ Cannot create root agent because one or more sub-agents failed to initialize or \\'get_weather\\' tool is missing.\")\\n    if not greeting_agent: print(\" - Greeting Agent is missing.\")\\n    if not farewell_agent: print(\" - Farewell Agent is missing.\")\\n    if \\'get_weather\\' not in globals(): print(\" - get_weather function is missing.\")',\n",
       "  'globals()',\n",
       "  'globals() # I am first time seeing this function - ig this gives all the attributes and methods in the python'],\n",
       " '_oh': {2: True, 15: {...}},\n",
       " '_dh': [PosixPath('/Users/badrinarayan/my_learning/google_adk/agent_adk_docs')],\n",
       " 'In': ['',\n",
       "  '# @title Import necessary libraries\\nimport os\\nimport asyncio\\nfrom google.adk.agents import Agent\\nfrom google.adk.models.lite_llm import LiteLlm # For multi-model support\\nfrom google.adk.sessions import InMemorySessionService\\nfrom google.adk.runners import Runner\\nfrom google.genai import types # For creating message Content/Parts\\n\\nimport warnings\\n# Ignore all warnings\\nwarnings.filterwarnings(\"ignore\")\\n\\nimport logging\\nlogging.basicConfig(level=logging.ERROR)\\n\\nprint(\"Libraries imported.\")',\n",
       "  'from dotenv import load_dotenv\\nload_dotenv()',\n",
       "  '# @title Configure API Keys (Replace with your actual keys!)\\n\\n# --- Verify Keys (Optional Check) ---\\nprint(\"API Keys Set:\")\\nprint(f\"Google API Key set: {\\'Yes\\' if os.environ.get(\\'GOOGLE_API_KEY\\') and os.environ[\\'GOOGLE_API_KEY\\'] != \\'YOUR_GOOGLE_API_KEY\\' else \\'No (REPLACE PLACEHOLDER!)\\'}\")\\nprint(f\"OpenAI API Key set: {\\'Yes\\' if os.environ.get(\\'OPENAI_API_KEY\\') and os.environ[\\'OPENAI_API_KEY\\'] != \\'YOUR_OPENAI_API_KEY\\' else \\'No (REPLACE PLACEHOLDER!)\\'}\")\\nprint(f\"Anthropic API Key set: {\\'Yes\\' if os.environ.get(\\'ANTHROPIC_API_KEY\\') and os.environ[\\'ANTHROPIC_API_KEY\\'] != \\'YOUR_ANTHROPIC_API_KEY\\' else \\'No (REPLACE PLACEHOLDER!)\\'}\")\\n\\n# Configure ADK to use API keys directly (not Vertex AI for this multi-model setup)\\nos.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = \"False\"\\n\\n\\n# @markdown **Security Note:** It\\'s best practice to manage API keys securely (e.g., using Colab Secrets or environment variables) rather than hardcoding them directly in the notebook. Replace the placeholder strings above.',\n",
       "  '# --- Define Model Constants for easier use ---\\n\\n# More supported models can be referenced here: https://ai.google.dev/gemini-api/docs/models#model-variations\\nMODEL_GEMINI_2_0_FLASH = \"gemini-2.0-flash\"\\n\\n# More supported models can be referenced here: https://docs.litellm.ai/docs/providers/openai#openai-chat-completion-models\\nMODEL_GPT_4O = \"openai/gpt-4.1-mini\" # You can also try: gpt-4.1-mini, gpt-4o etc.\\n\\n# More supported models can be referenced here: https://docs.litellm.ai/docs/providers/anthropic\\nMODEL_CLAUDE_SONNET = \"anthropic/claude-haiku-4-5-20251001\" # You can also try: claude-opus-4-20250514 , claude-3-7-sonnet-20250219 etc\\n\\nprint(\"\\\\nEnvironment configured.\")',\n",
       "  '# @title Define the get_weather Tool\\ndef get_weather(city: str) -> dict:\\n    \"\"\"Retrieves the current weather report for a specified city.\\n\\n    Args:\\n        city (str): The name of the city (e.g., \"New York\", \"London\", \"Tokyo\").\\n\\n    Returns:\\n        dict: A dictionary containing the weather information.\\n              Includes a \\'status\\' key (\\'success\\' or \\'error\\').\\n              If \\'success\\', includes a \\'report\\' key with weather details.\\n              If \\'error\\', includes an \\'error_message\\' key.\\n    \"\"\"\\n    print(f\"--- Tool: get_weather called for city: {city} ---\") # Log tool execution\\n    city_normalized = city.lower().replace(\" \", \"\") # Basic normalization\\n\\n    # Mock weather data\\n    mock_weather_db = {\\n        \"newyork\": {\"status\": \"success\", \"report\": \"The weather in New York is sunny with a temperature of 25°C.\"},\\n        \"london\": {\"status\": \"success\", \"report\": \"It\\'s cloudy in London with a temperature of 15°C.\"},\\n        \"tokyo\": {\"status\": \"success\", \"report\": \"Tokyo is experiencing light rain and a temperature of 18°C.\"},\\n    }\\n\\n    if city_normalized in mock_weather_db:\\n        return mock_weather_db[city_normalized]\\n    else:\\n        return {\"status\": \"error\", \"error_message\": f\"Sorry, I don\\'t have weather information for \\'{city}\\'.\"}\\n\\n# Example tool usage (optional test)\\nprint(get_weather(\"New York\"))\\nprint(get_weather(\"Paris\"))',\n",
       "  '# @title Define the Weather Agent\\n# Use one of the model constants defined earlier\\nAGENT_MODEL = MODEL_GEMINI_2_0_FLASH # Starting with Gemini\\n\\nweather_agent = Agent(\\n    name=\"weather_agent_v1\",\\n    model=AGENT_MODEL, # Can be a string for Gemini or a LiteLlm object\\n    description=\"Provides weather information for specific cities.\",\\n    instruction=\"You are a helpful weather assistant. \"\\n                \"When the user asks for the weather in a specific city, \"\\n                \"use the \\'get_weather\\' tool to find the information. \"\\n                \"If the tool returns an error, inform the user politely. \"\\n                \"If the tool is successful, present the weather report clearly.\",\\n    tools=[get_weather], # Pass the function directly\\n)\\n\\nprint(f\"Agent \\'{weather_agent.name}\\' created using model \\'{AGENT_MODEL}\\'.\")',\n",
       "  '# @title Setup Session Service and Runner\\n\\n# --- Session Management ---\\n# Key Concept: SessionService stores conversation history & state.\\n# InMemorySessionService is simple, non-persistent storage for this tutorial.\\nsession_service = InMemorySessionService()\\n\\n# Define constants for identifying the interaction context\\nAPP_NAME = \"weather_tutorial_app\"\\nUSER_ID = \"user_1\"\\nSESSION_ID = \"session_001\" # Using a fixed ID for simplicity\\n\\n# Create the specific session where the conversation will happen\\nsession = await session_service.create_session(\\n    app_name=APP_NAME,\\n    user_id=USER_ID,\\n    session_id=SESSION_ID\\n)\\nprint(f\"Session created: App=\\'{APP_NAME}\\', User=\\'{USER_ID}\\', Session=\\'{SESSION_ID}\\'\")\\n\\n# --- OR ---\\n\\n# Uncomment the following lines if running as a standard Python script (.py file):\\n\\n# async def init_session(app_name:str,user_id:str,session_id:str) -> InMemorySessionService:\\n#     session = await session_service.create_session(\\n#         app_name=app_name,\\n#         user_id=user_id,\\n#         session_id=session_id\\n#     )\\n#     print(f\"Session created: App=\\'{app_name}\\', User=\\'{user_id}\\', Session=\\'{session_id}\\'\")\\n#     return session\\n# \\n# session = asyncio.run(init_session(APP_NAME,USER_ID,SESSION_ID))\\n\\n# --- Runner ---\\n# Key Concept: Runner orchestrates the agent execution loop.\\nrunner = Runner(\\n    agent=weather_agent, # The agent we want to run\\n    app_name=APP_NAME,   # Associates runs with our app\\n    session_service=session_service # Uses our session manager\\n)\\nprint(f\"Runner created for agent \\'{runner.agent.name}\\'.\")',\n",
       "  '# @title Define Agent Interaction Function\\n\\nfrom google.genai import types # For creating message Content/Parts\\n\\nasync def call_agent_async(query: str, runner, user_id, session_id):\\n  \"\"\"Sends a query to the agent and prints the final response.\"\"\"\\n  print(f\"\\\\n>>> User Query: {query}\")\\n\\n  # Prepare the user\\'s message in ADK format\\n  content = types.Content(role=\\'user\\', parts=[types.Part(text=query)])\\n\\n  final_response_text = \"Agent did not produce a final response.\" # Default\\n\\n  # Key Concept: run_async executes the agent logic and yields Events.\\n  # We iterate through events to find the final answer.\\n  async for event in runner.run_async(user_id=user_id, session_id=session_id, new_message=content):\\n      # You can uncomment the line below to see *all* events during execution\\n      # print(f\"  [Event] Author: {event.author}, Type: {type(event).__name__}, Final: {event.is_final_response()}, Content: {event.content}\")\\n\\n      # Key Concept: is_final_response() marks the concluding message for the turn.\\n      if event.is_final_response():\\n          if event.content and event.content.parts:\\n             # Assuming text response in the first part\\n             final_response_text = event.content.parts[0].text\\n          elif event.actions and event.actions.escalate: # Handle potential errors/escalations\\n             final_response_text = f\"Agent escalated: {event.error_message or \\'No specific message.\\'}\"\\n          # Add more checks here if needed (e.g., specific error codes)\\n          break # Stop processing events once the final response is found\\n\\n  print(f\"<<< Agent Response: {final_response_text}\")',\n",
       "  '# @title Run the Initial Conversation\\n\\n# We need an async function to await our interaction helper\\nasync def run_conversation():\\n    await call_agent_async(\"What is the weather like in London?\",\\n                                       runner=runner,\\n                                       user_id=USER_ID,\\n                                       session_id=SESSION_ID)\\n\\n    await call_agent_async(\"How about Paris?\",\\n                                       runner=runner,\\n                                       user_id=USER_ID,\\n                                       session_id=SESSION_ID) # Expecting the tool\\'s error message\\n\\n    await call_agent_async(\"Tell me the weather in New York\",\\n                                       runner=runner,\\n                                       user_id=USER_ID,\\n                                       session_id=SESSION_ID)\\n\\n# Execute the conversation using await in an async context (like Colab/Jupyter)\\nawait run_conversation()\\n\\n# --- OR ---\\n\\n# Uncomment the following lines if running as a standard Python script (.py file):\\n# import asyncio\\n# if __name__ == \"__main__\":\\n#     try:\\n#         asyncio.run(run_conversation())\\n#     except Exception as e:\\n#         print(f\"An error occurred: {e}\")',\n",
       "  '# @title Define and Test GPT Agent\\n\\n# Make sure \\'get_weather\\' function from Step 1 is defined in your environment.\\n# Make sure \\'call_agent_async\\' is defined from earlier.\\n\\n# --- Agent using GPT-4o ---\\nweather_agent_gpt = None # Initialize to None\\nrunner_gpt = None      # Initialize runner to None\\n\\ntry:\\n    weather_agent_gpt = Agent(\\n        name=\"weather_agent_gpt\",\\n        # Key change: Wrap the LiteLLM model identifier\\n        model=LiteLlm(model=MODEL_GPT_4O),\\n        description=\"Provides weather information (using GPT-4o).\",\\n        instruction=\"You are a helpful weather assistant powered by GPT-4o. \"\\n                    \"Use the \\'get_weather\\' tool for city weather requests. \"\\n                    \"Clearly present successful reports or polite error messages based on the tool\\'s output status.\",\\n        tools=[get_weather], # Re-use the same tool\\n    )\\n    print(f\"Agent \\'{weather_agent_gpt.name}\\' created using model \\'{MODEL_GPT_4O}\\'.\")\\n\\n    # InMemorySessionService is simple, non-persistent storage for this tutorial.\\n    session_service_gpt = InMemorySessionService() # Create a dedicated service\\n\\n    # Define constants for identifying the interaction context\\n    APP_NAME_GPT = \"weather_tutorial_app_gpt\" # Unique app name for this test\\n    USER_ID_GPT = \"user_1_gpt\"\\n    SESSION_ID_GPT = \"session_001_gpt\" # Using a fixed ID for simplicity\\n\\n    # Create the specific session where the conversation will happen\\n    session_gpt = await session_service_gpt.create_session(\\n        app_name=APP_NAME_GPT,\\n        user_id=USER_ID_GPT,\\n        session_id=SESSION_ID_GPT\\n    )\\n    print(f\"Session created: App=\\'{APP_NAME_GPT}\\', User=\\'{USER_ID_GPT}\\', Session=\\'{SESSION_ID_GPT}\\'\")\\n\\n    # Create a runner specific to this agent and its session service\\n    runner_gpt = Runner(\\n        agent=weather_agent_gpt,\\n        app_name=APP_NAME_GPT,       # Use the specific app name\\n        session_service=session_service_gpt # Use the specific session service\\n        )\\n    print(f\"Runner created for agent \\'{runner_gpt.agent.name}\\'.\")\\n\\n    # --- Test the GPT Agent ---\\n    print(\"\\\\n--- Testing GPT Agent ---\")\\n    # Ensure call_agent_async uses the correct runner, user_id, session_id\\n    await call_agent_async(query = \"What\\'s the weather in Tokyo?\",\\n                           runner=runner_gpt,\\n                           user_id=USER_ID_GPT,\\n                           session_id=SESSION_ID_GPT)\\n    # --- OR ---\\n\\n    # Uncomment the following lines if running as a standard Python script (.py file):\\n    # import asyncio\\n    # if __name__ == \"__main__\":\\n    #     try:\\n    #         asyncio.run(call_agent_async(query = \"What\\'s the weather in Tokyo?\",\\n    #                      runner=runner_gpt,\\n    #                       user_id=USER_ID_GPT,\\n    #                       session_id=SESSION_ID_GPT)\\n    #     except Exception as e:\\n    #         print(f\"An error occurred: {e}\")\\n\\nexcept Exception as e:\\n    print(f\"❌ Could not create or run GPT agent \\'{MODEL_GPT_4O}\\'. Check API Key and model name. Error: {e}\")',\n",
       "  '# @title Define and Test Claude Agent\\n\\n# Make sure \\'get_weather\\' function from Step 1 is defined in your environment.\\n# Make sure \\'call_agent_async\\' is defined from earlier.\\n\\n# --- Agent using Claude Sonnet ---\\nweather_agent_claude = None # Initialize to None\\nrunner_claude = None      # Initialize runner to None\\n\\ntry:\\n    weather_agent_claude = Agent(\\n        name=\"weather_agent_claude\",\\n        # Key change: Wrap the LiteLLM model identifier\\n        model=LiteLlm(model=MODEL_CLAUDE_SONNET),\\n        description=\"Provides weather information (using Claude Sonnet).\",\\n        instruction=\"You are a helpful weather assistant powered by Claude Sonnet. \"\\n                    \"Use the \\'get_weather\\' tool for city weather requests. \"\\n                    \"Analyze the tool\\'s dictionary output (\\'status\\', \\'report\\'/\\'error_message\\'). \"\\n                    \"Clearly present successful reports or polite error messages.\",\\n        tools=[get_weather], # Re-use the same tool\\n    )\\n    print(f\"Agent \\'{weather_agent_claude.name}\\' created using model \\'{MODEL_CLAUDE_SONNET}\\'.\")\\n\\n    # InMemorySessionService is simple, non-persistent storage for this tutorial.\\n    session_service_claude = InMemorySessionService() # Create a dedicated service\\n\\n    # Define constants for identifying the interaction context\\n    APP_NAME_CLAUDE = \"weather_tutorial_app_claude\" # Unique app name\\n    USER_ID_CLAUDE = \"user_1_claude\"\\n    SESSION_ID_CLAUDE = \"session_001_claude\" # Using a fixed ID for simplicity\\n\\n    # Create the specific session where the conversation will happen\\n    session_claude = await session_service_claude.create_session(\\n        app_name=APP_NAME_CLAUDE,\\n        user_id=USER_ID_CLAUDE,\\n        session_id=SESSION_ID_CLAUDE\\n    )\\n    print(f\"Session created: App=\\'{APP_NAME_CLAUDE}\\', User=\\'{USER_ID_CLAUDE}\\', Session=\\'{SESSION_ID_CLAUDE}\\'\")\\n\\n    # Create a runner specific to this agent and its session service\\n    runner_claude = Runner(\\n        agent=weather_agent_claude,\\n        app_name=APP_NAME_CLAUDE,       # Use the specific app name\\n        session_service=session_service_claude # Use the specific session service\\n        )\\n    print(f\"Runner created for agent \\'{runner_claude.agent.name}\\'.\")\\n\\n    # --- Test the Claude Agent ---\\n    print(\"\\\\n--- Testing Claude Agent ---\")\\n    # Ensure call_agent_async uses the correct runner, user_id, session_id\\n    await call_agent_async(query = \"Weather in London please.\",\\n                           runner=runner_claude,\\n                           user_id=USER_ID_CLAUDE,\\n                           session_id=SESSION_ID_CLAUDE)\\n\\n    # --- OR ---\\n\\n    # Uncomment the following lines if running as a standard Python script (.py file):\\n    # import asyncio\\n    # if __name__ == \"__main__\":\\n    #     try:\\n    #         asyncio.run(call_agent_async(query = \"Weather in London please.\",\\n    #                      runner=runner_claude,\\n    #                       user_id=USER_ID_CLAUDE,\\n    #                       session_id=SESSION_ID_CLAUDE)\\n    #     except Exception as e:\\n    #         print(f\"An error occurred: {e}\")\\n\\n\\nexcept Exception as e:\\n    print(f\"❌ Could not create or run Claude agent \\'{MODEL_CLAUDE_SONNET}\\'. Check API Key and model name. Error: {e}\")',\n",
       "  '# @title Define Tools for Greeting and Farewell Agents\\nfrom typing import Optional # Make sure to import Optional\\n\\n# Ensure \\'get_weather\\' from Step 1 is available if running this step independently.\\n# def get_weather(city: str) -> dict: ... (from Step 1)\\n\\ndef say_hello(name: Optional[str] = None) -> str:\\n    \"\"\"Provides a simple greeting. If a name is provided, it will be used.\\n\\n    Args:\\n        name (str, optional): The name of the person to greet. Defaults to a generic greeting if not provided.\\n\\n    Returns:\\n        str: A friendly greeting message.\\n    \"\"\"\\n    if name:\\n        greeting = f\"Hello, {name}!\"\\n        print(f\"--- Tool: say_hello called with name: {name} ---\")\\n    else:\\n        greeting = \"Hello there!\" # Default greeting if name is None or not explicitly passed\\n        print(f\"--- Tool: say_hello called without a specific name (name_arg_value: {name}) ---\")\\n    return greeting\\n\\ndef say_goodbye() -> str:\\n    \"\"\"Provides a simple farewell message to conclude the conversation.\"\"\"\\n    print(f\"--- Tool: say_goodbye called ---\")\\n    return \"Goodbye! Have a great day.\"\\n\\nprint(\"Greeting and Farewell tools defined.\")\\n\\n# Optional self-test\\nprint(say_hello(\"Alice\"))\\nprint(say_hello()) # Test with no argument (should use default \"Hello there!\")\\nprint(say_hello(name=None)) # Test with name explicitly as None (should use default \"Hello there!\")',\n",
       "  '# @title Define Greeting and Farewell Sub-Agents\\n\\n# If you want to use models other than Gemini, Ensure LiteLlm is imported and API keys are set (from Step 0/2)\\n# from google.adk.models.lite_llm import LiteLlm\\n# MODEL_GPT_4O, MODEL_CLAUDE_SONNET etc. should be defined\\n# Or else, continue to use: model = MODEL_GEMINI_2_0_FLASH\\n\\n# --- Greeting Agent ---\\ngreeting_agent = None\\ntry:\\n    greeting_agent = Agent(\\n        # Using a potentially different/cheaper model for a simple task\\n        model = MODEL_GEMINI_2_0_FLASH,\\n        # model=LiteLlm(model=MODEL_GPT_4O), # If you would like to experiment with other models\\n        name=\"greeting_agent\",\\n        instruction=\"You are the Greeting Agent. Your ONLY task is to provide a friendly greeting to the user. \"\\n                    \"Use the \\'say_hello\\' tool to generate the greeting. \"\\n                    \"If the user provides their name, make sure to pass it to the tool. \"\\n                    \"Do not engage in any other conversation or tasks.\",\\n        description=\"Handles simple greetings and hellos using the \\'say_hello\\' tool.\", # Crucial for delegation\\n        tools=[say_hello],\\n    )\\n    print(f\"✅ Agent \\'{greeting_agent.name}\\' created using model \\'{greeting_agent.model}\\'.\")\\nexcept Exception as e:\\n    print(f\"❌ Could not create Greeting agent. Check API Key ({greeting_agent.model}). Error: {e}\")\\n\\n# --- Farewell Agent ---\\nfarewell_agent = None\\ntry:\\n    farewell_agent = Agent(\\n        # Can use the same or a different model\\n        model = MODEL_GEMINI_2_0_FLASH,\\n        # model=LiteLlm(model=MODEL_GPT_4O), # If you would like to experiment with other models\\n        name=\"farewell_agent\",\\n        instruction=\"You are the Farewell Agent. Your ONLY task is to provide a polite goodbye message. \"\\n                    \"Use the \\'say_goodbye\\' tool when the user indicates they are leaving or ending the conversation \"\\n                    \"(e.g., using words like \\'bye\\', \\'goodbye\\', \\'thanks bye\\', \\'see you\\'). \"\\n                    \"Do not perform any other actions.\",\\n        description=\"Handles simple farewells and goodbyes using the \\'say_goodbye\\' tool.\", # Crucial for delegation\\n        tools=[say_goodbye],\\n    )\\n    print(f\"✅ Agent \\'{farewell_agent.name}\\' created using model \\'{farewell_agent.model}\\'.\")\\nexcept Exception as e:\\n    print(f\"❌ Could not create Farewell agent. Check API Key ({farewell_agent.model}). Error: {e}\")',\n",
       "  '# @title Define the Root Agent with Sub-Agents\\n\\n# Ensure sub-agents were created successfully before defining the root agent.\\n# Also ensure the original \\'get_weather\\' tool is defined.\\nroot_agent = None\\nrunner_root = None # Initialize runner\\n\\nif greeting_agent and farewell_agent and \\'get_weather\\' in globals():\\n    # Let\\'s use a capable Gemini model for the root agent to handle orchestration\\n    root_agent_model = MODEL_GEMINI_2_0_FLASH\\n\\n    weather_agent_team = Agent(\\n        name=\"weather_agent_v2\", # Give it a new version name\\n        model=root_agent_model,\\n        description=\"The main coordinator agent. Handles weather requests and delegates greetings/farewells to specialists.\",\\n        instruction=\"You are the main Weather Agent coordinating a team. Your primary responsibility is to provide weather information. \"\\n                    \"Use the \\'get_weather\\' tool ONLY for specific weather requests (e.g., \\'weather in London\\'). \"\\n                    \"You have specialized sub-agents: \"\\n                    \"1. \\'greeting_agent\\': Handles simple greetings like \\'Hi\\', \\'Hello\\'. Delegate to it for these. \"\\n                    \"2. \\'farewell_agent\\': Handles simple farewells like \\'Bye\\', \\'See you\\'. Delegate to it for these. \"\\n                    \"Analyze the user\\'s query. If it\\'s a greeting, delegate to \\'greeting_agent\\'. If it\\'s a farewell, delegate to \\'farewell_agent\\'. \"\\n                    \"If it\\'s a weather request, handle it yourself using \\'get_weather\\'. \"\\n                    \"For anything else, respond appropriately or state you cannot handle it.\",\\n        tools=[get_weather], # Root agent still needs the weather tool for its core task\\n        # Key change: Link the sub-agents here!\\n        sub_agents=[greeting_agent, farewell_agent]\\n    )\\n    print(f\"✅ Root Agent \\'{weather_agent_team.name}\\' created using model \\'{root_agent_model}\\' with sub-agents: {[sa.name for sa in weather_agent_team.sub_agents]}\")\\n\\nelse:\\n    print(\"❌ Cannot create root agent because one or more sub-agents failed to initialize or \\'get_weather\\' tool is missing.\")\\n    if not greeting_agent: print(\" - Greeting Agent is missing.\")\\n    if not farewell_agent: print(\" - Farewell Agent is missing.\")\\n    if \\'get_weather\\' not in globals(): print(\" - get_weather function is missing.\")',\n",
       "  'globals()',\n",
       "  'globals() # I am first time seeing this function - ig this gives all the attributes and methods in the python'],\n",
       " 'Out': {2: True, 15: {...}},\n",
       " 'get_ipython': <bound method InteractiveShell.get_ipython of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x1083c0110>>,\n",
       " 'exit': <IPython.core.autocall.ZMQExitAutocall at 0x1083cc7d0>,\n",
       " 'quit': <IPython.core.autocall.ZMQExitAutocall at 0x1083cc7d0>,\n",
       " 'open': <function io.open(file, mode='r', buffering=-1, encoding=None, errors=None, newline=None, closefd=True, opener=None)>,\n",
       " '_': {...},\n",
       " '__': True,\n",
       " '___': '',\n",
       " '__session__': '/Users/badrinarayan/my_learning/google_adk/agent_adk_docs/progressive_weather_bot-dce1bc6b-50e3-4ef8-a9ff-f7e5063e4d6b.ipynb',\n",
       " '__vsc_ipynb_file__': '/Users/badrinarayan/my_learning/google_adk/agent_adk_docs/progressive_weather_bot.ipynb',\n",
       " '_i': 'globals()',\n",
       " '_ii': '# @title Define the Root Agent with Sub-Agents\\n\\n# Ensure sub-agents were created successfully before defining the root agent.\\n# Also ensure the original \\'get_weather\\' tool is defined.\\nroot_agent = None\\nrunner_root = None # Initialize runner\\n\\nif greeting_agent and farewell_agent and \\'get_weather\\' in globals():\\n    # Let\\'s use a capable Gemini model for the root agent to handle orchestration\\n    root_agent_model = MODEL_GEMINI_2_0_FLASH\\n\\n    weather_agent_team = Agent(\\n        name=\"weather_agent_v2\", # Give it a new version name\\n        model=root_agent_model,\\n        description=\"The main coordinator agent. Handles weather requests and delegates greetings/farewells to specialists.\",\\n        instruction=\"You are the main Weather Agent coordinating a team. Your primary responsibility is to provide weather information. \"\\n                    \"Use the \\'get_weather\\' tool ONLY for specific weather requests (e.g., \\'weather in London\\'). \"\\n                    \"You have specialized sub-agents: \"\\n                    \"1. \\'greeting_agent\\': Handles simple greetings like \\'Hi\\', \\'Hello\\'. Delegate to it for these. \"\\n                    \"2. \\'farewell_agent\\': Handles simple farewells like \\'Bye\\', \\'See you\\'. Delegate to it for these. \"\\n                    \"Analyze the user\\'s query. If it\\'s a greeting, delegate to \\'greeting_agent\\'. If it\\'s a farewell, delegate to \\'farewell_agent\\'. \"\\n                    \"If it\\'s a weather request, handle it yourself using \\'get_weather\\'. \"\\n                    \"For anything else, respond appropriately or state you cannot handle it.\",\\n        tools=[get_weather], # Root agent still needs the weather tool for its core task\\n        # Key change: Link the sub-agents here!\\n        sub_agents=[greeting_agent, farewell_agent]\\n    )\\n    print(f\"✅ Root Agent \\'{weather_agent_team.name}\\' created using model \\'{root_agent_model}\\' with sub-agents: {[sa.name for sa in weather_agent_team.sub_agents]}\")\\n\\nelse:\\n    print(\"❌ Cannot create root agent because one or more sub-agents failed to initialize or \\'get_weather\\' tool is missing.\")\\n    if not greeting_agent: print(\" - Greeting Agent is missing.\")\\n    if not farewell_agent: print(\" - Farewell Agent is missing.\")\\n    if \\'get_weather\\' not in globals(): print(\" - get_weather function is missing.\")',\n",
       " '_iii': '# @title Define Greeting and Farewell Sub-Agents\\n\\n# If you want to use models other than Gemini, Ensure LiteLlm is imported and API keys are set (from Step 0/2)\\n# from google.adk.models.lite_llm import LiteLlm\\n# MODEL_GPT_4O, MODEL_CLAUDE_SONNET etc. should be defined\\n# Or else, continue to use: model = MODEL_GEMINI_2_0_FLASH\\n\\n# --- Greeting Agent ---\\ngreeting_agent = None\\ntry:\\n    greeting_agent = Agent(\\n        # Using a potentially different/cheaper model for a simple task\\n        model = MODEL_GEMINI_2_0_FLASH,\\n        # model=LiteLlm(model=MODEL_GPT_4O), # If you would like to experiment with other models\\n        name=\"greeting_agent\",\\n        instruction=\"You are the Greeting Agent. Your ONLY task is to provide a friendly greeting to the user. \"\\n                    \"Use the \\'say_hello\\' tool to generate the greeting. \"\\n                    \"If the user provides their name, make sure to pass it to the tool. \"\\n                    \"Do not engage in any other conversation or tasks.\",\\n        description=\"Handles simple greetings and hellos using the \\'say_hello\\' tool.\", # Crucial for delegation\\n        tools=[say_hello],\\n    )\\n    print(f\"✅ Agent \\'{greeting_agent.name}\\' created using model \\'{greeting_agent.model}\\'.\")\\nexcept Exception as e:\\n    print(f\"❌ Could not create Greeting agent. Check API Key ({greeting_agent.model}). Error: {e}\")\\n\\n# --- Farewell Agent ---\\nfarewell_agent = None\\ntry:\\n    farewell_agent = Agent(\\n        # Can use the same or a different model\\n        model = MODEL_GEMINI_2_0_FLASH,\\n        # model=LiteLlm(model=MODEL_GPT_4O), # If you would like to experiment with other models\\n        name=\"farewell_agent\",\\n        instruction=\"You are the Farewell Agent. Your ONLY task is to provide a polite goodbye message. \"\\n                    \"Use the \\'say_goodbye\\' tool when the user indicates they are leaving or ending the conversation \"\\n                    \"(e.g., using words like \\'bye\\', \\'goodbye\\', \\'thanks bye\\', \\'see you\\'). \"\\n                    \"Do not perform any other actions.\",\\n        description=\"Handles simple farewells and goodbyes using the \\'say_goodbye\\' tool.\", # Crucial for delegation\\n        tools=[say_goodbye],\\n    )\\n    print(f\"✅ Agent \\'{farewell_agent.name}\\' created using model \\'{farewell_agent.model}\\'.\")\\nexcept Exception as e:\\n    print(f\"❌ Could not create Farewell agent. Check API Key ({farewell_agent.model}). Error: {e}\")',\n",
       " '_i1': '# @title Import necessary libraries\\nimport os\\nimport asyncio\\nfrom google.adk.agents import Agent\\nfrom google.adk.models.lite_llm import LiteLlm # For multi-model support\\nfrom google.adk.sessions import InMemorySessionService\\nfrom google.adk.runners import Runner\\nfrom google.genai import types # For creating message Content/Parts\\n\\nimport warnings\\n# Ignore all warnings\\nwarnings.filterwarnings(\"ignore\")\\n\\nimport logging\\nlogging.basicConfig(level=logging.ERROR)\\n\\nprint(\"Libraries imported.\")',\n",
       " 'os': <module 'os' (frozen)>,\n",
       " 'asyncio': <module 'asyncio' from '/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/__init__.py'>,\n",
       " 'Agent': google.adk.agents.llm_agent.LlmAgent,\n",
       " 'LiteLlm': google.adk.models.lite_llm.LiteLlm,\n",
       " 'InMemorySessionService': google.adk.sessions.in_memory_session_service.InMemorySessionService,\n",
       " 'Runner': google.adk.runners.Runner,\n",
       " 'types': <module 'google.genai.types' from '/Users/badrinarayan/my_learning/google_adk/google-adk/lib/python3.11/site-packages/google/genai/types.py'>,\n",
       " 'warnings': <module 'warnings' from '/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/warnings.py'>,\n",
       " 'logging': <module 'logging' from '/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/logging/__init__.py'>,\n",
       " '_i2': 'from dotenv import load_dotenv\\nload_dotenv()',\n",
       " 'load_dotenv': <function dotenv.main.load_dotenv(dotenv_path: Union[str, ForwardRef('os.PathLike[str]'), NoneType] = None, stream: Optional[IO[str]] = None, verbose: bool = False, override: bool = False, interpolate: bool = True, encoding: Optional[str] = 'utf-8') -> bool>,\n",
       " '_2': True,\n",
       " '_i3': '# @title Configure API Keys (Replace with your actual keys!)\\n\\n# --- Verify Keys (Optional Check) ---\\nprint(\"API Keys Set:\")\\nprint(f\"Google API Key set: {\\'Yes\\' if os.environ.get(\\'GOOGLE_API_KEY\\') and os.environ[\\'GOOGLE_API_KEY\\'] != \\'YOUR_GOOGLE_API_KEY\\' else \\'No (REPLACE PLACEHOLDER!)\\'}\")\\nprint(f\"OpenAI API Key set: {\\'Yes\\' if os.environ.get(\\'OPENAI_API_KEY\\') and os.environ[\\'OPENAI_API_KEY\\'] != \\'YOUR_OPENAI_API_KEY\\' else \\'No (REPLACE PLACEHOLDER!)\\'}\")\\nprint(f\"Anthropic API Key set: {\\'Yes\\' if os.environ.get(\\'ANTHROPIC_API_KEY\\') and os.environ[\\'ANTHROPIC_API_KEY\\'] != \\'YOUR_ANTHROPIC_API_KEY\\' else \\'No (REPLACE PLACEHOLDER!)\\'}\")\\n\\n# Configure ADK to use API keys directly (not Vertex AI for this multi-model setup)\\nos.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = \"False\"\\n\\n\\n# @markdown **Security Note:** It\\'s best practice to manage API keys securely (e.g., using Colab Secrets or environment variables) rather than hardcoding them directly in the notebook. Replace the placeholder strings above.',\n",
       " '_i4': '# --- Define Model Constants for easier use ---\\n\\n# More supported models can be referenced here: https://ai.google.dev/gemini-api/docs/models#model-variations\\nMODEL_GEMINI_2_0_FLASH = \"gemini-2.0-flash\"\\n\\n# More supported models can be referenced here: https://docs.litellm.ai/docs/providers/openai#openai-chat-completion-models\\nMODEL_GPT_4O = \"openai/gpt-4.1-mini\" # You can also try: gpt-4.1-mini, gpt-4o etc.\\n\\n# More supported models can be referenced here: https://docs.litellm.ai/docs/providers/anthropic\\nMODEL_CLAUDE_SONNET = \"anthropic/claude-haiku-4-5-20251001\" # You can also try: claude-opus-4-20250514 , claude-3-7-sonnet-20250219 etc\\n\\nprint(\"\\\\nEnvironment configured.\")',\n",
       " 'MODEL_GEMINI_2_0_FLASH': 'gemini-2.0-flash',\n",
       " 'MODEL_GPT_4O': 'openai/gpt-4.1-mini',\n",
       " 'MODEL_CLAUDE_SONNET': 'anthropic/claude-haiku-4-5-20251001',\n",
       " '_i5': '# @title Define the get_weather Tool\\ndef get_weather(city: str) -> dict:\\n    \"\"\"Retrieves the current weather report for a specified city.\\n\\n    Args:\\n        city (str): The name of the city (e.g., \"New York\", \"London\", \"Tokyo\").\\n\\n    Returns:\\n        dict: A dictionary containing the weather information.\\n              Includes a \\'status\\' key (\\'success\\' or \\'error\\').\\n              If \\'success\\', includes a \\'report\\' key with weather details.\\n              If \\'error\\', includes an \\'error_message\\' key.\\n    \"\"\"\\n    print(f\"--- Tool: get_weather called for city: {city} ---\") # Log tool execution\\n    city_normalized = city.lower().replace(\" \", \"\") # Basic normalization\\n\\n    # Mock weather data\\n    mock_weather_db = {\\n        \"newyork\": {\"status\": \"success\", \"report\": \"The weather in New York is sunny with a temperature of 25°C.\"},\\n        \"london\": {\"status\": \"success\", \"report\": \"It\\'s cloudy in London with a temperature of 15°C.\"},\\n        \"tokyo\": {\"status\": \"success\", \"report\": \"Tokyo is experiencing light rain and a temperature of 18°C.\"},\\n    }\\n\\n    if city_normalized in mock_weather_db:\\n        return mock_weather_db[city_normalized]\\n    else:\\n        return {\"status\": \"error\", \"error_message\": f\"Sorry, I don\\'t have weather information for \\'{city}\\'.\"}\\n\\n# Example tool usage (optional test)\\nprint(get_weather(\"New York\"))\\nprint(get_weather(\"Paris\"))',\n",
       " 'get_weather': <function __main__.get_weather(city: str) -> dict>,\n",
       " '_i6': '# @title Define the Weather Agent\\n# Use one of the model constants defined earlier\\nAGENT_MODEL = MODEL_GEMINI_2_0_FLASH # Starting with Gemini\\n\\nweather_agent = Agent(\\n    name=\"weather_agent_v1\",\\n    model=AGENT_MODEL, # Can be a string for Gemini or a LiteLlm object\\n    description=\"Provides weather information for specific cities.\",\\n    instruction=\"You are a helpful weather assistant. \"\\n                \"When the user asks for the weather in a specific city, \"\\n                \"use the \\'get_weather\\' tool to find the information. \"\\n                \"If the tool returns an error, inform the user politely. \"\\n                \"If the tool is successful, present the weather report clearly.\",\\n    tools=[get_weather], # Pass the function directly\\n)\\n\\nprint(f\"Agent \\'{weather_agent.name}\\' created using model \\'{AGENT_MODEL}\\'.\")',\n",
       " 'AGENT_MODEL': 'gemini-2.0-flash',\n",
       " 'weather_agent': LlmAgent(name='weather_agent_v1', description='Provides weather information for specific cities.', parent_agent=None, sub_agents=[], before_agent_callback=None, after_agent_callback=None, model='gemini-2.0-flash', instruction=\"You are a helpful weather assistant. When the user asks for the weather in a specific city, use the 'get_weather' tool to find the information. If the tool returns an error, inform the user politely. If the tool is successful, present the weather report clearly.\", global_instruction='', static_instruction=None, tools=[<function get_weather at 0x116742d40>], generate_content_config=None, disallow_transfer_to_parent=False, disallow_transfer_to_peers=False, include_contents='default', input_schema=None, output_schema=None, output_key=None, planner=None, code_executor=None, before_model_callback=None, after_model_callback=None, on_model_error_callback=None, before_tool_callback=None, after_tool_callback=None, on_tool_error_callback=None),\n",
       " '_i7': '# @title Setup Session Service and Runner\\n\\n# --- Session Management ---\\n# Key Concept: SessionService stores conversation history & state.\\n# InMemorySessionService is simple, non-persistent storage for this tutorial.\\nsession_service = InMemorySessionService()\\n\\n# Define constants for identifying the interaction context\\nAPP_NAME = \"weather_tutorial_app\"\\nUSER_ID = \"user_1\"\\nSESSION_ID = \"session_001\" # Using a fixed ID for simplicity\\n\\n# Create the specific session where the conversation will happen\\nsession = await session_service.create_session(\\n    app_name=APP_NAME,\\n    user_id=USER_ID,\\n    session_id=SESSION_ID\\n)\\nprint(f\"Session created: App=\\'{APP_NAME}\\', User=\\'{USER_ID}\\', Session=\\'{SESSION_ID}\\'\")\\n\\n# --- OR ---\\n\\n# Uncomment the following lines if running as a standard Python script (.py file):\\n\\n# async def init_session(app_name:str,user_id:str,session_id:str) -> InMemorySessionService:\\n#     session = await session_service.create_session(\\n#         app_name=app_name,\\n#         user_id=user_id,\\n#         session_id=session_id\\n#     )\\n#     print(f\"Session created: App=\\'{app_name}\\', User=\\'{user_id}\\', Session=\\'{session_id}\\'\")\\n#     return session\\n# \\n# session = asyncio.run(init_session(APP_NAME,USER_ID,SESSION_ID))\\n\\n# --- Runner ---\\n# Key Concept: Runner orchestrates the agent execution loop.\\nrunner = Runner(\\n    agent=weather_agent, # The agent we want to run\\n    app_name=APP_NAME,   # Associates runs with our app\\n    session_service=session_service # Uses our session manager\\n)\\nprint(f\"Runner created for agent \\'{runner.agent.name}\\'.\")',\n",
       " 'session_service': <google.adk.sessions.in_memory_session_service.InMemorySessionService at 0x116790c50>,\n",
       " 'APP_NAME': 'weather_tutorial_app',\n",
       " 'USER_ID': 'user_1',\n",
       " 'SESSION_ID': 'session_001',\n",
       " 'session': Session(id='session_001', app_name='weather_tutorial_app', user_id='user_1', state={}, events=[], last_update_time=1764171459.5543308),\n",
       " 'runner': <google.adk.runners.Runner at 0x10840fe10>,\n",
       " '_i8': '# @title Define Agent Interaction Function\\n\\nfrom google.genai import types # For creating message Content/Parts\\n\\nasync def call_agent_async(query: str, runner, user_id, session_id):\\n  \"\"\"Sends a query to the agent and prints the final response.\"\"\"\\n  print(f\"\\\\n>>> User Query: {query}\")\\n\\n  # Prepare the user\\'s message in ADK format\\n  content = types.Content(role=\\'user\\', parts=[types.Part(text=query)])\\n\\n  final_response_text = \"Agent did not produce a final response.\" # Default\\n\\n  # Key Concept: run_async executes the agent logic and yields Events.\\n  # We iterate through events to find the final answer.\\n  async for event in runner.run_async(user_id=user_id, session_id=session_id, new_message=content):\\n      # You can uncomment the line below to see *all* events during execution\\n      # print(f\"  [Event] Author: {event.author}, Type: {type(event).__name__}, Final: {event.is_final_response()}, Content: {event.content}\")\\n\\n      # Key Concept: is_final_response() marks the concluding message for the turn.\\n      if event.is_final_response():\\n          if event.content and event.content.parts:\\n             # Assuming text response in the first part\\n             final_response_text = event.content.parts[0].text\\n          elif event.actions and event.actions.escalate: # Handle potential errors/escalations\\n             final_response_text = f\"Agent escalated: {event.error_message or \\'No specific message.\\'}\"\\n          # Add more checks here if needed (e.g., specific error codes)\\n          break # Stop processing events once the final response is found\\n\\n  print(f\"<<< Agent Response: {final_response_text}\")',\n",
       " 'call_agent_async': <function __main__.call_agent_async(query: str, runner, user_id, session_id)>,\n",
       " '_i9': '# @title Run the Initial Conversation\\n\\n# We need an async function to await our interaction helper\\nasync def run_conversation():\\n    await call_agent_async(\"What is the weather like in London?\",\\n                                       runner=runner,\\n                                       user_id=USER_ID,\\n                                       session_id=SESSION_ID)\\n\\n    await call_agent_async(\"How about Paris?\",\\n                                       runner=runner,\\n                                       user_id=USER_ID,\\n                                       session_id=SESSION_ID) # Expecting the tool\\'s error message\\n\\n    await call_agent_async(\"Tell me the weather in New York\",\\n                                       runner=runner,\\n                                       user_id=USER_ID,\\n                                       session_id=SESSION_ID)\\n\\n# Execute the conversation using await in an async context (like Colab/Jupyter)\\nawait run_conversation()\\n\\n# --- OR ---\\n\\n# Uncomment the following lines if running as a standard Python script (.py file):\\n# import asyncio\\n# if __name__ == \"__main__\":\\n#     try:\\n#         asyncio.run(run_conversation())\\n#     except Exception as e:\\n#         print(f\"An error occurred: {e}\")',\n",
       " 'run_conversation': <function __main__.run_conversation()>,\n",
       " '_i10': '# @title Define and Test GPT Agent\\n\\n# Make sure \\'get_weather\\' function from Step 1 is defined in your environment.\\n# Make sure \\'call_agent_async\\' is defined from earlier.\\n\\n# --- Agent using GPT-4o ---\\nweather_agent_gpt = None # Initialize to None\\nrunner_gpt = None      # Initialize runner to None\\n\\ntry:\\n    weather_agent_gpt = Agent(\\n        name=\"weather_agent_gpt\",\\n        # Key change: Wrap the LiteLLM model identifier\\n        model=LiteLlm(model=MODEL_GPT_4O),\\n        description=\"Provides weather information (using GPT-4o).\",\\n        instruction=\"You are a helpful weather assistant powered by GPT-4o. \"\\n                    \"Use the \\'get_weather\\' tool for city weather requests. \"\\n                    \"Clearly present successful reports or polite error messages based on the tool\\'s output status.\",\\n        tools=[get_weather], # Re-use the same tool\\n    )\\n    print(f\"Agent \\'{weather_agent_gpt.name}\\' created using model \\'{MODEL_GPT_4O}\\'.\")\\n\\n    # InMemorySessionService is simple, non-persistent storage for this tutorial.\\n    session_service_gpt = InMemorySessionService() # Create a dedicated service\\n\\n    # Define constants for identifying the interaction context\\n    APP_NAME_GPT = \"weather_tutorial_app_gpt\" # Unique app name for this test\\n    USER_ID_GPT = \"user_1_gpt\"\\n    SESSION_ID_GPT = \"session_001_gpt\" # Using a fixed ID for simplicity\\n\\n    # Create the specific session where the conversation will happen\\n    session_gpt = await session_service_gpt.create_session(\\n        app_name=APP_NAME_GPT,\\n        user_id=USER_ID_GPT,\\n        session_id=SESSION_ID_GPT\\n    )\\n    print(f\"Session created: App=\\'{APP_NAME_GPT}\\', User=\\'{USER_ID_GPT}\\', Session=\\'{SESSION_ID_GPT}\\'\")\\n\\n    # Create a runner specific to this agent and its session service\\n    runner_gpt = Runner(\\n        agent=weather_agent_gpt,\\n        app_name=APP_NAME_GPT,       # Use the specific app name\\n        session_service=session_service_gpt # Use the specific session service\\n        )\\n    print(f\"Runner created for agent \\'{runner_gpt.agent.name}\\'.\")\\n\\n    # --- Test the GPT Agent ---\\n    print(\"\\\\n--- Testing GPT Agent ---\")\\n    # Ensure call_agent_async uses the correct runner, user_id, session_id\\n    await call_agent_async(query = \"What\\'s the weather in Tokyo?\",\\n                           runner=runner_gpt,\\n                           user_id=USER_ID_GPT,\\n                           session_id=SESSION_ID_GPT)\\n    # --- OR ---\\n\\n    # Uncomment the following lines if running as a standard Python script (.py file):\\n    # import asyncio\\n    # if __name__ == \"__main__\":\\n    #     try:\\n    #         asyncio.run(call_agent_async(query = \"What\\'s the weather in Tokyo?\",\\n    #                      runner=runner_gpt,\\n    #                       user_id=USER_ID_GPT,\\n    #                       session_id=SESSION_ID_GPT)\\n    #     except Exception as e:\\n    #         print(f\"An error occurred: {e}\")\\n\\nexcept Exception as e:\\n    print(f\"❌ Could not create or run GPT agent \\'{MODEL_GPT_4O}\\'. Check API Key and model name. Error: {e}\")',\n",
       " 'weather_agent_gpt': LlmAgent(name='weather_agent_gpt', description='Provides weather information (using GPT-4o).', parent_agent=None, sub_agents=[], before_agent_callback=None, after_agent_callback=None, model=LiteLlm(model='openai/gpt-4.1-mini', llm_client=<google.adk.models.lite_llm.LiteLLMClient object at 0x116791150>), instruction=\"You are a helpful weather assistant powered by GPT-4o. Use the 'get_weather' tool for city weather requests. Clearly present successful reports or polite error messages based on the tool's output status.\", global_instruction='', static_instruction=None, tools=[<function get_weather at 0x116742d40>], generate_content_config=None, disallow_transfer_to_parent=False, disallow_transfer_to_peers=False, include_contents='default', input_schema=None, output_schema=None, output_key=None, planner=None, code_executor=None, before_model_callback=None, after_model_callback=None, on_model_error_callback=None, before_tool_callback=None, after_tool_callback=None, on_tool_error_callback=None),\n",
       " 'runner_gpt': <google.adk.runners.Runner at 0x116764310>,\n",
       " 'session_service_gpt': <google.adk.sessions.in_memory_session_service.InMemorySessionService at 0x107f4c450>,\n",
       " 'APP_NAME_GPT': 'weather_tutorial_app_gpt',\n",
       " 'USER_ID_GPT': 'user_1_gpt',\n",
       " 'SESSION_ID_GPT': 'session_001_gpt',\n",
       " 'session_gpt': Session(id='session_001_gpt', app_name='weather_tutorial_app_gpt', user_id='user_1_gpt', state={}, events=[], last_update_time=1764172542.8824),\n",
       " '_i11': '# @title Define and Test Claude Agent\\n\\n# Make sure \\'get_weather\\' function from Step 1 is defined in your environment.\\n# Make sure \\'call_agent_async\\' is defined from earlier.\\n\\n# --- Agent using Claude Sonnet ---\\nweather_agent_claude = None # Initialize to None\\nrunner_claude = None      # Initialize runner to None\\n\\ntry:\\n    weather_agent_claude = Agent(\\n        name=\"weather_agent_claude\",\\n        # Key change: Wrap the LiteLLM model identifier\\n        model=LiteLlm(model=MODEL_CLAUDE_SONNET),\\n        description=\"Provides weather information (using Claude Sonnet).\",\\n        instruction=\"You are a helpful weather assistant powered by Claude Sonnet. \"\\n                    \"Use the \\'get_weather\\' tool for city weather requests. \"\\n                    \"Analyze the tool\\'s dictionary output (\\'status\\', \\'report\\'/\\'error_message\\'). \"\\n                    \"Clearly present successful reports or polite error messages.\",\\n        tools=[get_weather], # Re-use the same tool\\n    )\\n    print(f\"Agent \\'{weather_agent_claude.name}\\' created using model \\'{MODEL_CLAUDE_SONNET}\\'.\")\\n\\n    # InMemorySessionService is simple, non-persistent storage for this tutorial.\\n    session_service_claude = InMemorySessionService() # Create a dedicated service\\n\\n    # Define constants for identifying the interaction context\\n    APP_NAME_CLAUDE = \"weather_tutorial_app_claude\" # Unique app name\\n    USER_ID_CLAUDE = \"user_1_claude\"\\n    SESSION_ID_CLAUDE = \"session_001_claude\" # Using a fixed ID for simplicity\\n\\n    # Create the specific session where the conversation will happen\\n    session_claude = await session_service_claude.create_session(\\n        app_name=APP_NAME_CLAUDE,\\n        user_id=USER_ID_CLAUDE,\\n        session_id=SESSION_ID_CLAUDE\\n    )\\n    print(f\"Session created: App=\\'{APP_NAME_CLAUDE}\\', User=\\'{USER_ID_CLAUDE}\\', Session=\\'{SESSION_ID_CLAUDE}\\'\")\\n\\n    # Create a runner specific to this agent and its session service\\n    runner_claude = Runner(\\n        agent=weather_agent_claude,\\n        app_name=APP_NAME_CLAUDE,       # Use the specific app name\\n        session_service=session_service_claude # Use the specific session service\\n        )\\n    print(f\"Runner created for agent \\'{runner_claude.agent.name}\\'.\")\\n\\n    # --- Test the Claude Agent ---\\n    print(\"\\\\n--- Testing Claude Agent ---\")\\n    # Ensure call_agent_async uses the correct runner, user_id, session_id\\n    await call_agent_async(query = \"Weather in London please.\",\\n                           runner=runner_claude,\\n                           user_id=USER_ID_CLAUDE,\\n                           session_id=SESSION_ID_CLAUDE)\\n\\n    # --- OR ---\\n\\n    # Uncomment the following lines if running as a standard Python script (.py file):\\n    # import asyncio\\n    # if __name__ == \"__main__\":\\n    #     try:\\n    #         asyncio.run(call_agent_async(query = \"Weather in London please.\",\\n    #                      runner=runner_claude,\\n    #                       user_id=USER_ID_CLAUDE,\\n    #                       session_id=SESSION_ID_CLAUDE)\\n    #     except Exception as e:\\n    #         print(f\"An error occurred: {e}\")\\n\\n\\nexcept Exception as e:\\n    print(f\"❌ Could not create or run Claude agent \\'{MODEL_CLAUDE_SONNET}\\'. Check API Key and model name. Error: {e}\")',\n",
       " 'weather_agent_claude': LlmAgent(name='weather_agent_claude', description='Provides weather information (using Claude Sonnet).', parent_agent=None, sub_agents=[], before_agent_callback=None, after_agent_callback=None, model=LiteLlm(model='anthropic/claude-haiku-4-5-20251001', llm_client=<google.adk.models.lite_llm.LiteLLMClient object at 0x116764ad0>), instruction=\"You are a helpful weather assistant powered by Claude Sonnet. Use the 'get_weather' tool for city weather requests. Analyze the tool's dictionary output ('status', 'report'/'error_message'). Clearly present successful reports or polite error messages.\", global_instruction='', static_instruction=None, tools=[<function get_weather at 0x116742d40>], generate_content_config=None, disallow_transfer_to_parent=False, disallow_transfer_to_peers=False, include_contents='default', input_schema=None, output_schema=None, output_key=None, planner=None, code_executor=None, before_model_callback=None, after_model_callback=None, on_model_error_callback=None, before_tool_callback=None, after_tool_callback=None, on_tool_error_callback=None),\n",
       " 'runner_claude': <google.adk.runners.Runner at 0x1167ad950>,\n",
       " 'session_service_claude': <google.adk.sessions.in_memory_session_service.InMemorySessionService at 0x11678b450>,\n",
       " 'APP_NAME_CLAUDE': 'weather_tutorial_app_claude',\n",
       " 'USER_ID_CLAUDE': 'user_1_claude',\n",
       " 'SESSION_ID_CLAUDE': 'session_001_claude',\n",
       " 'session_claude': Session(id='session_001_claude', app_name='weather_tutorial_app_claude', user_id='user_1_claude', state={}, events=[], last_update_time=1764172572.69469),\n",
       " '_i12': '# @title Define Tools for Greeting and Farewell Agents\\nfrom typing import Optional # Make sure to import Optional\\n\\n# Ensure \\'get_weather\\' from Step 1 is available if running this step independently.\\n# def get_weather(city: str) -> dict: ... (from Step 1)\\n\\ndef say_hello(name: Optional[str] = None) -> str:\\n    \"\"\"Provides a simple greeting. If a name is provided, it will be used.\\n\\n    Args:\\n        name (str, optional): The name of the person to greet. Defaults to a generic greeting if not provided.\\n\\n    Returns:\\n        str: A friendly greeting message.\\n    \"\"\"\\n    if name:\\n        greeting = f\"Hello, {name}!\"\\n        print(f\"--- Tool: say_hello called with name: {name} ---\")\\n    else:\\n        greeting = \"Hello there!\" # Default greeting if name is None or not explicitly passed\\n        print(f\"--- Tool: say_hello called without a specific name (name_arg_value: {name}) ---\")\\n    return greeting\\n\\ndef say_goodbye() -> str:\\n    \"\"\"Provides a simple farewell message to conclude the conversation.\"\"\"\\n    print(f\"--- Tool: say_goodbye called ---\")\\n    return \"Goodbye! Have a great day.\"\\n\\nprint(\"Greeting and Farewell tools defined.\")\\n\\n# Optional self-test\\nprint(say_hello(\"Alice\"))\\nprint(say_hello()) # Test with no argument (should use default \"Hello there!\")\\nprint(say_hello(name=None)) # Test with name explicitly as None (should use default \"Hello there!\")',\n",
       " 'Optional': typing.Optional,\n",
       " 'say_hello': <function __main__.say_hello(name: Optional[str] = None) -> str>,\n",
       " 'say_goodbye': <function __main__.say_goodbye() -> str>,\n",
       " '_i13': '# @title Define Greeting and Farewell Sub-Agents\\n\\n# If you want to use models other than Gemini, Ensure LiteLlm is imported and API keys are set (from Step 0/2)\\n# from google.adk.models.lite_llm import LiteLlm\\n# MODEL_GPT_4O, MODEL_CLAUDE_SONNET etc. should be defined\\n# Or else, continue to use: model = MODEL_GEMINI_2_0_FLASH\\n\\n# --- Greeting Agent ---\\ngreeting_agent = None\\ntry:\\n    greeting_agent = Agent(\\n        # Using a potentially different/cheaper model for a simple task\\n        model = MODEL_GEMINI_2_0_FLASH,\\n        # model=LiteLlm(model=MODEL_GPT_4O), # If you would like to experiment with other models\\n        name=\"greeting_agent\",\\n        instruction=\"You are the Greeting Agent. Your ONLY task is to provide a friendly greeting to the user. \"\\n                    \"Use the \\'say_hello\\' tool to generate the greeting. \"\\n                    \"If the user provides their name, make sure to pass it to the tool. \"\\n                    \"Do not engage in any other conversation or tasks.\",\\n        description=\"Handles simple greetings and hellos using the \\'say_hello\\' tool.\", # Crucial for delegation\\n        tools=[say_hello],\\n    )\\n    print(f\"✅ Agent \\'{greeting_agent.name}\\' created using model \\'{greeting_agent.model}\\'.\")\\nexcept Exception as e:\\n    print(f\"❌ Could not create Greeting agent. Check API Key ({greeting_agent.model}). Error: {e}\")\\n\\n# --- Farewell Agent ---\\nfarewell_agent = None\\ntry:\\n    farewell_agent = Agent(\\n        # Can use the same or a different model\\n        model = MODEL_GEMINI_2_0_FLASH,\\n        # model=LiteLlm(model=MODEL_GPT_4O), # If you would like to experiment with other models\\n        name=\"farewell_agent\",\\n        instruction=\"You are the Farewell Agent. Your ONLY task is to provide a polite goodbye message. \"\\n                    \"Use the \\'say_goodbye\\' tool when the user indicates they are leaving or ending the conversation \"\\n                    \"(e.g., using words like \\'bye\\', \\'goodbye\\', \\'thanks bye\\', \\'see you\\'). \"\\n                    \"Do not perform any other actions.\",\\n        description=\"Handles simple farewells and goodbyes using the \\'say_goodbye\\' tool.\", # Crucial for delegation\\n        tools=[say_goodbye],\\n    )\\n    print(f\"✅ Agent \\'{farewell_agent.name}\\' created using model \\'{farewell_agent.model}\\'.\")\\nexcept Exception as e:\\n    print(f\"❌ Could not create Farewell agent. Check API Key ({farewell_agent.model}). Error: {e}\")',\n",
       " 'greeting_agent': LlmAgent(name='greeting_agent', description=\"Handles simple greetings and hellos using the 'say_hello' tool.\", parent_agent=LlmAgent(name='weather_agent_v2', description='The main coordinator agent. Handles weather requests and delegates greetings/farewells to specialists.', parent_agent=None, sub_agents=[LlmAgent(name='greeting_agent', description=\"Handles simple greetings and hellos using the 'say_hello' tool.\", parent_agent=LlmAgent(name='weather_agent_v2', description='The main coordinator agent. Handles weather requests and delegates greetings/farewells to specialists.', parent_agent=None, sub_agents=[...], before_agent_callback=None, after_agent_callback=None, model='gemini-2.0-flash', instruction=\"You are the main Weather Agent coordinating a team. Your primary responsibility is to provide weather information. Use the 'get_weather' tool ONLY for specific weather requests (e.g., 'weather in London'). You have specialized sub-agents: 1. 'greeting_agent': Handles simple greetings like 'Hi', 'Hello'. Delegate to it for these. 2. 'farewell_agent': Handles simple farewells like 'Bye', 'See you'. Delegate to it for these. Analyze the user's query. If it's a greeting, delegate to 'greeting_agent'. If it's a farewell, delegate to 'farewell_agent'. If it's a weather request, handle it yourself using 'get_weather'. For anything else, respond appropriately or state you cannot handle it.\", global_instruction='', static_instruction=None, tools=[<function get_weather at 0x116742d40>], generate_content_config=None, disallow_transfer_to_parent=False, disallow_transfer_to_peers=False, include_contents='default', input_schema=None, output_schema=None, output_key=None, planner=None, code_executor=None, before_model_callback=None, after_model_callback=None, on_model_error_callback=None, before_tool_callback=None, after_tool_callback=None, on_tool_error_callback=None), sub_agents=[], before_agent_callback=None, after_agent_callback=None, model='gemini-2.0-flash', instruction=\"You are the Greeting Agent. Your ONLY task is to provide a friendly greeting to the user. Use the 'say_hello' tool to generate the greeting. If the user provides their name, make sure to pass it to the tool. Do not engage in any other conversation or tasks.\", global_instruction='', static_instruction=None, tools=[<function say_hello at 0x111eae700>], generate_content_config=None, disallow_transfer_to_parent=False, disallow_transfer_to_peers=False, include_contents='default', input_schema=None, output_schema=None, output_key=None, planner=None, code_executor=None, before_model_callback=None, after_model_callback=None, on_model_error_callback=None, before_tool_callback=None, after_tool_callback=None, on_tool_error_callback=None), LlmAgent(name='farewell_agent', description=\"Handles simple farewells and goodbyes using the 'say_goodbye' tool.\", parent_agent=LlmAgent(name='weather_agent_v2', description='The main coordinator agent. Handles weather requests and delegates greetings/farewells to specialists.', parent_agent=None, sub_agents=[...], before_agent_callback=None, after_agent_callback=None, model='gemini-2.0-flash', instruction=\"You are the main Weather Agent coordinating a team. Your primary responsibility is to provide weather information. Use the 'get_weather' tool ONLY for specific weather requests (e.g., 'weather in London'). You have specialized sub-agents: 1. 'greeting_agent': Handles simple greetings like 'Hi', 'Hello'. Delegate to it for these. 2. 'farewell_agent': Handles simple farewells like 'Bye', 'See you'. Delegate to it for these. Analyze the user's query. If it's a greeting, delegate to 'greeting_agent'. If it's a farewell, delegate to 'farewell_agent'. If it's a weather request, handle it yourself using 'get_weather'. For anything else, respond appropriately or state you cannot handle it.\", global_instruction='', static_instruction=None, tools=[<function get_weather at 0x116742d40>], generate_content_config=None, disallow_transfer_to_parent=False, disallow_transfer_to_peers=False, include_contents='default', input_schema=None, output_schema=None, output_key=None, planner=None, code_executor=None, before_model_callback=None, after_model_callback=None, on_model_error_callback=None, before_tool_callback=None, after_tool_callback=None, on_tool_error_callback=None), sub_agents=[], before_agent_callback=None, after_agent_callback=None, model='gemini-2.0-flash', instruction=\"You are the Farewell Agent. Your ONLY task is to provide a polite goodbye message. Use the 'say_goodbye' tool when the user indicates they are leaving or ending the conversation (e.g., using words like 'bye', 'goodbye', 'thanks bye', 'see you'). Do not perform any other actions.\", global_instruction='', static_instruction=None, tools=[<function say_goodbye at 0x1171258a0>], generate_content_config=None, disallow_transfer_to_parent=False, disallow_transfer_to_peers=False, include_contents='default', input_schema=None, output_schema=None, output_key=None, planner=None, code_executor=None, before_model_callback=None, after_model_callback=None, on_model_error_callback=None, before_tool_callback=None, after_tool_callback=None, on_tool_error_callback=None)], before_agent_callback=None, after_agent_callback=None, model='gemini-2.0-flash', instruction=\"You are the main Weather Agent coordinating a team. Your primary responsibility is to provide weather information. Use the 'get_weather' tool ONLY for specific weather requests (e.g., 'weather in London'). You have specialized sub-agents: 1. 'greeting_agent': Handles simple greetings like 'Hi', 'Hello'. Delegate to it for these. 2. 'farewell_agent': Handles simple farewells like 'Bye', 'See you'. Delegate to it for these. Analyze the user's query. If it's a greeting, delegate to 'greeting_agent'. If it's a farewell, delegate to 'farewell_agent'. If it's a weather request, handle it yourself using 'get_weather'. For anything else, respond appropriately or state you cannot handle it.\", global_instruction='', static_instruction=None, tools=[<function get_weather at 0x116742d40>], generate_content_config=None, disallow_transfer_to_parent=False, disallow_transfer_to_peers=False, include_contents='default', input_schema=None, output_schema=None, output_key=None, planner=None, code_executor=None, before_model_callback=None, after_model_callback=None, on_model_error_callback=None, before_tool_callback=None, after_tool_callback=None, on_tool_error_callback=None), sub_agents=[], before_agent_callback=None, after_agent_callback=None, model='gemini-2.0-flash', instruction=\"You are the Greeting Agent. Your ONLY task is to provide a friendly greeting to the user. Use the 'say_hello' tool to generate the greeting. If the user provides their name, make sure to pass it to the tool. Do not engage in any other conversation or tasks.\", global_instruction='', static_instruction=None, tools=[<function say_hello at 0x111eae700>], generate_content_config=None, disallow_transfer_to_parent=False, disallow_transfer_to_peers=False, include_contents='default', input_schema=None, output_schema=None, output_key=None, planner=None, code_executor=None, before_model_callback=None, after_model_callback=None, on_model_error_callback=None, before_tool_callback=None, after_tool_callback=None, on_tool_error_callback=None),\n",
       " 'farewell_agent': LlmAgent(name='farewell_agent', description=\"Handles simple farewells and goodbyes using the 'say_goodbye' tool.\", parent_agent=LlmAgent(name='weather_agent_v2', description='The main coordinator agent. Handles weather requests and delegates greetings/farewells to specialists.', parent_agent=None, sub_agents=[LlmAgent(name='greeting_agent', description=\"Handles simple greetings and hellos using the 'say_hello' tool.\", parent_agent=LlmAgent(name='weather_agent_v2', description='The main coordinator agent. Handles weather requests and delegates greetings/farewells to specialists.', parent_agent=None, sub_agents=[...], before_agent_callback=None, after_agent_callback=None, model='gemini-2.0-flash', instruction=\"You are the main Weather Agent coordinating a team. Your primary responsibility is to provide weather information. Use the 'get_weather' tool ONLY for specific weather requests (e.g., 'weather in London'). You have specialized sub-agents: 1. 'greeting_agent': Handles simple greetings like 'Hi', 'Hello'. Delegate to it for these. 2. 'farewell_agent': Handles simple farewells like 'Bye', 'See you'. Delegate to it for these. Analyze the user's query. If it's a greeting, delegate to 'greeting_agent'. If it's a farewell, delegate to 'farewell_agent'. If it's a weather request, handle it yourself using 'get_weather'. For anything else, respond appropriately or state you cannot handle it.\", global_instruction='', static_instruction=None, tools=[<function get_weather at 0x116742d40>], generate_content_config=None, disallow_transfer_to_parent=False, disallow_transfer_to_peers=False, include_contents='default', input_schema=None, output_schema=None, output_key=None, planner=None, code_executor=None, before_model_callback=None, after_model_callback=None, on_model_error_callback=None, before_tool_callback=None, after_tool_callback=None, on_tool_error_callback=None), sub_agents=[], before_agent_callback=None, after_agent_callback=None, model='gemini-2.0-flash', instruction=\"You are the Greeting Agent. Your ONLY task is to provide a friendly greeting to the user. Use the 'say_hello' tool to generate the greeting. If the user provides their name, make sure to pass it to the tool. Do not engage in any other conversation or tasks.\", global_instruction='', static_instruction=None, tools=[<function say_hello at 0x111eae700>], generate_content_config=None, disallow_transfer_to_parent=False, disallow_transfer_to_peers=False, include_contents='default', input_schema=None, output_schema=None, output_key=None, planner=None, code_executor=None, before_model_callback=None, after_model_callback=None, on_model_error_callback=None, before_tool_callback=None, after_tool_callback=None, on_tool_error_callback=None), LlmAgent(name='farewell_agent', description=\"Handles simple farewells and goodbyes using the 'say_goodbye' tool.\", parent_agent=LlmAgent(name='weather_agent_v2', description='The main coordinator agent. Handles weather requests and delegates greetings/farewells to specialists.', parent_agent=None, sub_agents=[...], before_agent_callback=None, after_agent_callback=None, model='gemini-2.0-flash', instruction=\"You are the main Weather Agent coordinating a team. Your primary responsibility is to provide weather information. Use the 'get_weather' tool ONLY for specific weather requests (e.g., 'weather in London'). You have specialized sub-agents: 1. 'greeting_agent': Handles simple greetings like 'Hi', 'Hello'. Delegate to it for these. 2. 'farewell_agent': Handles simple farewells like 'Bye', 'See you'. Delegate to it for these. Analyze the user's query. If it's a greeting, delegate to 'greeting_agent'. If it's a farewell, delegate to 'farewell_agent'. If it's a weather request, handle it yourself using 'get_weather'. For anything else, respond appropriately or state you cannot handle it.\", global_instruction='', static_instruction=None, tools=[<function get_weather at 0x116742d40>], generate_content_config=None, disallow_transfer_to_parent=False, disallow_transfer_to_peers=False, include_contents='default', input_schema=None, output_schema=None, output_key=None, planner=None, code_executor=None, before_model_callback=None, after_model_callback=None, on_model_error_callback=None, before_tool_callback=None, after_tool_callback=None, on_tool_error_callback=None), sub_agents=[], before_agent_callback=None, after_agent_callback=None, model='gemini-2.0-flash', instruction=\"You are the Farewell Agent. Your ONLY task is to provide a polite goodbye message. Use the 'say_goodbye' tool when the user indicates they are leaving or ending the conversation (e.g., using words like 'bye', 'goodbye', 'thanks bye', 'see you'). Do not perform any other actions.\", global_instruction='', static_instruction=None, tools=[<function say_goodbye at 0x1171258a0>], generate_content_config=None, disallow_transfer_to_parent=False, disallow_transfer_to_peers=False, include_contents='default', input_schema=None, output_schema=None, output_key=None, planner=None, code_executor=None, before_model_callback=None, after_model_callback=None, on_model_error_callback=None, before_tool_callback=None, after_tool_callback=None, on_tool_error_callback=None)], before_agent_callback=None, after_agent_callback=None, model='gemini-2.0-flash', instruction=\"You are the main Weather Agent coordinating a team. Your primary responsibility is to provide weather information. Use the 'get_weather' tool ONLY for specific weather requests (e.g., 'weather in London'). You have specialized sub-agents: 1. 'greeting_agent': Handles simple greetings like 'Hi', 'Hello'. Delegate to it for these. 2. 'farewell_agent': Handles simple farewells like 'Bye', 'See you'. Delegate to it for these. Analyze the user's query. If it's a greeting, delegate to 'greeting_agent'. If it's a farewell, delegate to 'farewell_agent'. If it's a weather request, handle it yourself using 'get_weather'. For anything else, respond appropriately or state you cannot handle it.\", global_instruction='', static_instruction=None, tools=[<function get_weather at 0x116742d40>], generate_content_config=None, disallow_transfer_to_parent=False, disallow_transfer_to_peers=False, include_contents='default', input_schema=None, output_schema=None, output_key=None, planner=None, code_executor=None, before_model_callback=None, after_model_callback=None, on_model_error_callback=None, before_tool_callback=None, after_tool_callback=None, on_tool_error_callback=None), sub_agents=[], before_agent_callback=None, after_agent_callback=None, model='gemini-2.0-flash', instruction=\"You are the Farewell Agent. Your ONLY task is to provide a polite goodbye message. Use the 'say_goodbye' tool when the user indicates they are leaving or ending the conversation (e.g., using words like 'bye', 'goodbye', 'thanks bye', 'see you'). Do not perform any other actions.\", global_instruction='', static_instruction=None, tools=[<function say_goodbye at 0x1171258a0>], generate_content_config=None, disallow_transfer_to_parent=False, disallow_transfer_to_peers=False, include_contents='default', input_schema=None, output_schema=None, output_key=None, planner=None, code_executor=None, before_model_callback=None, after_model_callback=None, on_model_error_callback=None, before_tool_callback=None, after_tool_callback=None, on_tool_error_callback=None),\n",
       " '_i14': '# @title Define the Root Agent with Sub-Agents\\n\\n# Ensure sub-agents were created successfully before defining the root agent.\\n# Also ensure the original \\'get_weather\\' tool is defined.\\nroot_agent = None\\nrunner_root = None # Initialize runner\\n\\nif greeting_agent and farewell_agent and \\'get_weather\\' in globals():\\n    # Let\\'s use a capable Gemini model for the root agent to handle orchestration\\n    root_agent_model = MODEL_GEMINI_2_0_FLASH\\n\\n    weather_agent_team = Agent(\\n        name=\"weather_agent_v2\", # Give it a new version name\\n        model=root_agent_model,\\n        description=\"The main coordinator agent. Handles weather requests and delegates greetings/farewells to specialists.\",\\n        instruction=\"You are the main Weather Agent coordinating a team. Your primary responsibility is to provide weather information. \"\\n                    \"Use the \\'get_weather\\' tool ONLY for specific weather requests (e.g., \\'weather in London\\'). \"\\n                    \"You have specialized sub-agents: \"\\n                    \"1. \\'greeting_agent\\': Handles simple greetings like \\'Hi\\', \\'Hello\\'. Delegate to it for these. \"\\n                    \"2. \\'farewell_agent\\': Handles simple farewells like \\'Bye\\', \\'See you\\'. Delegate to it for these. \"\\n                    \"Analyze the user\\'s query. If it\\'s a greeting, delegate to \\'greeting_agent\\'. If it\\'s a farewell, delegate to \\'farewell_agent\\'. \"\\n                    \"If it\\'s a weather request, handle it yourself using \\'get_weather\\'. \"\\n                    \"For anything else, respond appropriately or state you cannot handle it.\",\\n        tools=[get_weather], # Root agent still needs the weather tool for its core task\\n        # Key change: Link the sub-agents here!\\n        sub_agents=[greeting_agent, farewell_agent]\\n    )\\n    print(f\"✅ Root Agent \\'{weather_agent_team.name}\\' created using model \\'{root_agent_model}\\' with sub-agents: {[sa.name for sa in weather_agent_team.sub_agents]}\")\\n\\nelse:\\n    print(\"❌ Cannot create root agent because one or more sub-agents failed to initialize or \\'get_weather\\' tool is missing.\")\\n    if not greeting_agent: print(\" - Greeting Agent is missing.\")\\n    if not farewell_agent: print(\" - Farewell Agent is missing.\")\\n    if \\'get_weather\\' not in globals(): print(\" - get_weather function is missing.\")',\n",
       " 'root_agent': None,\n",
       " 'runner_root': None,\n",
       " 'root_agent_model': 'gemini-2.0-flash',\n",
       " 'weather_agent_team': LlmAgent(name='weather_agent_v2', description='The main coordinator agent. Handles weather requests and delegates greetings/farewells to specialists.', parent_agent=None, sub_agents=[LlmAgent(name='greeting_agent', description=\"Handles simple greetings and hellos using the 'say_hello' tool.\", parent_agent=LlmAgent(name='weather_agent_v2', description='The main coordinator agent. Handles weather requests and delegates greetings/farewells to specialists.', parent_agent=None, sub_agents=[...], before_agent_callback=None, after_agent_callback=None, model='gemini-2.0-flash', instruction=\"You are the main Weather Agent coordinating a team. Your primary responsibility is to provide weather information. Use the 'get_weather' tool ONLY for specific weather requests (e.g., 'weather in London'). You have specialized sub-agents: 1. 'greeting_agent': Handles simple greetings like 'Hi', 'Hello'. Delegate to it for these. 2. 'farewell_agent': Handles simple farewells like 'Bye', 'See you'. Delegate to it for these. Analyze the user's query. If it's a greeting, delegate to 'greeting_agent'. If it's a farewell, delegate to 'farewell_agent'. If it's a weather request, handle it yourself using 'get_weather'. For anything else, respond appropriately or state you cannot handle it.\", global_instruction='', static_instruction=None, tools=[<function get_weather at 0x116742d40>], generate_content_config=None, disallow_transfer_to_parent=False, disallow_transfer_to_peers=False, include_contents='default', input_schema=None, output_schema=None, output_key=None, planner=None, code_executor=None, before_model_callback=None, after_model_callback=None, on_model_error_callback=None, before_tool_callback=None, after_tool_callback=None, on_tool_error_callback=None), sub_agents=[], before_agent_callback=None, after_agent_callback=None, model='gemini-2.0-flash', instruction=\"You are the Greeting Agent. Your ONLY task is to provide a friendly greeting to the user. Use the 'say_hello' tool to generate the greeting. If the user provides their name, make sure to pass it to the tool. Do not engage in any other conversation or tasks.\", global_instruction='', static_instruction=None, tools=[<function say_hello at 0x111eae700>], generate_content_config=None, disallow_transfer_to_parent=False, disallow_transfer_to_peers=False, include_contents='default', input_schema=None, output_schema=None, output_key=None, planner=None, code_executor=None, before_model_callback=None, after_model_callback=None, on_model_error_callback=None, before_tool_callback=None, after_tool_callback=None, on_tool_error_callback=None), LlmAgent(name='farewell_agent', description=\"Handles simple farewells and goodbyes using the 'say_goodbye' tool.\", parent_agent=LlmAgent(name='weather_agent_v2', description='The main coordinator agent. Handles weather requests and delegates greetings/farewells to specialists.', parent_agent=None, sub_agents=[...], before_agent_callback=None, after_agent_callback=None, model='gemini-2.0-flash', instruction=\"You are the main Weather Agent coordinating a team. Your primary responsibility is to provide weather information. Use the 'get_weather' tool ONLY for specific weather requests (e.g., 'weather in London'). You have specialized sub-agents: 1. 'greeting_agent': Handles simple greetings like 'Hi', 'Hello'. Delegate to it for these. 2. 'farewell_agent': Handles simple farewells like 'Bye', 'See you'. Delegate to it for these. Analyze the user's query. If it's a greeting, delegate to 'greeting_agent'. If it's a farewell, delegate to 'farewell_agent'. If it's a weather request, handle it yourself using 'get_weather'. For anything else, respond appropriately or state you cannot handle it.\", global_instruction='', static_instruction=None, tools=[<function get_weather at 0x116742d40>], generate_content_config=None, disallow_transfer_to_parent=False, disallow_transfer_to_peers=False, include_contents='default', input_schema=None, output_schema=None, output_key=None, planner=None, code_executor=None, before_model_callback=None, after_model_callback=None, on_model_error_callback=None, before_tool_callback=None, after_tool_callback=None, on_tool_error_callback=None), sub_agents=[], before_agent_callback=None, after_agent_callback=None, model='gemini-2.0-flash', instruction=\"You are the Farewell Agent. Your ONLY task is to provide a polite goodbye message. Use the 'say_goodbye' tool when the user indicates they are leaving or ending the conversation (e.g., using words like 'bye', 'goodbye', 'thanks bye', 'see you'). Do not perform any other actions.\", global_instruction='', static_instruction=None, tools=[<function say_goodbye at 0x1171258a0>], generate_content_config=None, disallow_transfer_to_parent=False, disallow_transfer_to_peers=False, include_contents='default', input_schema=None, output_schema=None, output_key=None, planner=None, code_executor=None, before_model_callback=None, after_model_callback=None, on_model_error_callback=None, before_tool_callback=None, after_tool_callback=None, on_tool_error_callback=None)], before_agent_callback=None, after_agent_callback=None, model='gemini-2.0-flash', instruction=\"You are the main Weather Agent coordinating a team. Your primary responsibility is to provide weather information. Use the 'get_weather' tool ONLY for specific weather requests (e.g., 'weather in London'). You have specialized sub-agents: 1. 'greeting_agent': Handles simple greetings like 'Hi', 'Hello'. Delegate to it for these. 2. 'farewell_agent': Handles simple farewells like 'Bye', 'See you'. Delegate to it for these. Analyze the user's query. If it's a greeting, delegate to 'greeting_agent'. If it's a farewell, delegate to 'farewell_agent'. If it's a weather request, handle it yourself using 'get_weather'. For anything else, respond appropriately or state you cannot handle it.\", global_instruction='', static_instruction=None, tools=[<function get_weather at 0x116742d40>], generate_content_config=None, disallow_transfer_to_parent=False, disallow_transfer_to_peers=False, include_contents='default', input_schema=None, output_schema=None, output_key=None, planner=None, code_executor=None, before_model_callback=None, after_model_callback=None, on_model_error_callback=None, before_tool_callback=None, after_tool_callback=None, on_tool_error_callback=None),\n",
       " '_i15': 'globals()',\n",
       " '_15': {...},\n",
       " '_i16': 'globals() # I am first time seeing this function - ig this gives all the attributes and methods in the python'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globals() # I am first time seeing this function - ig this gives all the attributes and methods in the python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68d31ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting execution using 'await' (default for notebooks)...\n",
      "\n",
      "--- Testing Agent Team Delegation ---\n",
      "Session created: App='weather_tutorial_agent_team', User='user_1_agent_team', Session='session_001_agent_team'\n",
      "Runner created for agent 'weather_agent_v2'.\n",
      "\n",
      ">>> User Query: Hello there!\n",
      "--- Tool: say_hello called without a specific name (name_arg_value: None) ---\n",
      "<<< Agent Response: Hello there!\n",
      "\n",
      "\n",
      ">>> User Query: What is the weather in New York?\n",
      "--- Tool: get_weather called for city: New York ---\n",
      "<<< Agent Response: The weather in New York is sunny with a temperature of 25°C.\n",
      "\n",
      "\n",
      ">>> User Query: Thanks, bye!\n",
      "--- Tool: say_goodbye called ---\n",
      "<<< Agent Response: Goodbye! Have a great day.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title Interact with the Agent Team\n",
    "import asyncio # Ensure asyncio is imported\n",
    "\n",
    "# Ensure the root agent (e.g., 'weather_agent_team' or 'root_agent' from the previous cell) is defined.\n",
    "# Ensure the call_agent_async function is defined.\n",
    "\n",
    "# Check if the root agent variable exists before defining the conversation function\n",
    "root_agent_var_name = 'root_agent' # Default name from Step 3 guide\n",
    "if 'weather_agent_team' in globals(): # Check if user used this name instead\n",
    "    root_agent_var_name = 'weather_agent_team'\n",
    "elif 'root_agent' not in globals():\n",
    "    print(\"⚠️ Root agent ('root_agent' or 'weather_agent_team') not found. Cannot define run_team_conversation.\")\n",
    "    # Assign a dummy value to prevent NameError later if the code block runs anyway\n",
    "    root_agent = None # Or set a flag to prevent execution\n",
    "\n",
    "# Only define and run if the root agent exists\n",
    "if root_agent_var_name in globals() and globals()[root_agent_var_name]:\n",
    "    # Define the main async function for the conversation logic.\n",
    "    # The 'await' keywords INSIDE this function are necessary for async operations.\n",
    "    async def run_team_conversation():\n",
    "        print(\"\\n--- Testing Agent Team Delegation ---\")\n",
    "        session_service = InMemorySessionService()\n",
    "        APP_NAME = \"weather_tutorial_agent_team\"\n",
    "        USER_ID = \"user_1_agent_team\"\n",
    "        SESSION_ID = \"session_001_agent_team\"\n",
    "        session = await session_service.create_session(\n",
    "            app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID\n",
    "        )\n",
    "        print(f\"Session created: App='{APP_NAME}', User='{USER_ID}', Session='{SESSION_ID}'\")\n",
    "\n",
    "        actual_root_agent = globals()[root_agent_var_name]\n",
    "        runner_agent_team = Runner( # Or use InMemoryRunner\n",
    "            agent=actual_root_agent,\n",
    "            app_name=APP_NAME,\n",
    "            session_service=session_service\n",
    "        )\n",
    "        print(f\"Runner created for agent '{actual_root_agent.name}'.\")\n",
    "\n",
    "        # --- Interactions using await (correct within async def) ---\n",
    "        await call_agent_async(query = \"Hello there!\",\n",
    "                               runner=runner_agent_team,\n",
    "                               user_id=USER_ID,\n",
    "                               session_id=SESSION_ID)\n",
    "        await call_agent_async(query = \"What is the weather in New York?\",\n",
    "                               runner=runner_agent_team,\n",
    "                               user_id=USER_ID,\n",
    "                               session_id=SESSION_ID)\n",
    "        await call_agent_async(query = \"Thanks, bye!\",\n",
    "                               runner=runner_agent_team,\n",
    "                               user_id=USER_ID,\n",
    "                               session_id=SESSION_ID)\n",
    "\n",
    "    # --- Execute the `run_team_conversation` async function ---\n",
    "    # Choose ONE of the methods below based on your environment.\n",
    "    # Note: This may require API keys for the models used!\n",
    "\n",
    "    # METHOD 1: Direct await (Default for Notebooks/Async REPLs)\n",
    "    # If your environment supports top-level await (like Colab/Jupyter notebooks),\n",
    "    # it means an event loop is already running, so you can directly await the function.\n",
    "    print(\"Attempting execution using 'await' (default for notebooks)...\")\n",
    "    await run_team_conversation()\n",
    "\n",
    "    # METHOD 2: asyncio.run (For Standard Python Scripts [.py])\n",
    "    # If running this code as a standard Python script from your terminal,\n",
    "    # the script context is synchronous. `asyncio.run()` is needed to\n",
    "    # create and manage an event loop to execute your async function.\n",
    "    # To use this method:\n",
    "    # 1. Comment out the `await run_team_conversation()` line above.\n",
    "    # 2. Uncomment the following block:\n",
    "    \"\"\"\n",
    "    import asyncio\n",
    "    if __name__ == \"__main__\": # Ensures this runs only when script is executed directly\n",
    "        print(\"Executing using 'asyncio.run()' (for standard Python scripts)...\")\n",
    "        try:\n",
    "            # This creates an event loop, runs your async function, and closes the loop.\n",
    "            asyncio.run(run_team_conversation())\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "    \"\"\"\n",
    "\n",
    "else:\n",
    "    # This message prints if the root agent variable wasn't found earlier\n",
    "    print(\"\\n⚠️ Skipping agent team conversation execution as the root agent was not successfully defined in a previous step.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50797d4",
   "metadata": {},
   "source": [
    "## Adding Memory and Personalization with Session State"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c437353",
   "metadata": {},
   "source": [
    "So far, our agent team can handle different tasks through delegation, but each interaction starts fresh – the agents have no memory of past conversations or user preferences within a session. To create more sophisticated and context-aware experiences, agents need memory. ADK provides this through Session State."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9735d960",
   "metadata": {},
   "source": [
    "What is Session State?\n",
    "\n",
    "    It's a Python dictionary (session.state) tied to a specific user session (identified by APP_NAME, USER_ID, SESSION_ID).\n",
    "\n",
    "    It persists information across multiple conversational turns within that session.\n",
    "\n",
    "    Agents and Tools can read from and write to this state, allowing them to remember details, adapt behavior, and personalize responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66baad86",
   "metadata": {},
   "source": [
    "How Agents Interact with State:\n",
    "\n",
    "    ToolContext (Primary Method): Tools can accept a ToolContext object (automatically provided by ADK if declared as the last argument). This object gives direct access to the session state via tool_context.state, allowing tools to read preferences or save results during execution.\n",
    "\n",
    "    output_key (Auto-Save Agent Response): An Agent can be configured with an output_key=\"your_key\". ADK will then automatically save the agent's final textual response for a turn into session.state[\"your_key\"]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "604ffa48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ New InMemorySessionService created for state demonstration.\n",
      "✅ Session 'session_state_demo_001' created for user 'user_state_demo'.\n",
      "\n",
      "--- Initial Session State ---\n",
      "{'user_preference_temperature_unit': 'Celsius'}\n"
     ]
    }
   ],
   "source": [
    "# @title 1. Initialize New Session Service and State\n",
    "\n",
    "# Import necessary session components\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "\n",
    "# Create a NEW session service instance for this state demonstration\n",
    "session_service_stateful = InMemorySessionService()\n",
    "print(\"✅ New InMemorySessionService created for state demonstration.\")\n",
    "\n",
    "# Define a NEW session ID for this part of the tutorial\n",
    "SESSION_ID_STATEFUL = \"session_state_demo_001\"\n",
    "USER_ID_STATEFUL = \"user_state_demo\"\n",
    "\n",
    "# Define initial state data - user prefers Celsius initially\n",
    "initial_state = {\n",
    "    \"user_preference_temperature_unit\": \"Celsius\"\n",
    "}\n",
    "\n",
    "# Create the session, providing the initial state\n",
    "session_stateful = await session_service_stateful.create_session(\n",
    "    app_name=APP_NAME, # Use the consistent app name\n",
    "    user_id=USER_ID_STATEFUL,\n",
    "    session_id=SESSION_ID_STATEFUL,\n",
    "    state=initial_state # <<< Initialize state during creation\n",
    ")\n",
    "print(f\"✅ Session '{SESSION_ID_STATEFUL}' created for user '{USER_ID_STATEFUL}'.\")\n",
    "\n",
    "# Verify the initial state was set correctly\n",
    "retrieved_session = await session_service_stateful.get_session(app_name=APP_NAME,\n",
    "                                                         user_id=USER_ID_STATEFUL,\n",
    "                                                         session_id = SESSION_ID_STATEFUL)\n",
    "print(\"\\n--- Initial Session State ---\")\n",
    "if retrieved_session:\n",
    "    print(retrieved_session.state)\n",
    "else:\n",
    "    print(\"Error: Could not retrieve session.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff36ad5",
   "metadata": {},
   "source": [
    "Ig session_service_stateful is prolly a vector database... This should be similar to Langmem..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a09ac7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ State-aware 'get_weather_stateful' tool defined.\n"
     ]
    }
   ],
   "source": [
    "from google.adk.tools.tool_context import ToolContext\n",
    "\n",
    "def get_weather_stateful(city: str, tool_context: ToolContext) -> dict:\n",
    "    \"\"\"Retrieves weather, converts temp unit based on session state.\"\"\"\n",
    "    print(f\"--- Tool: get_weather_stateful called for {city} ---\")\n",
    "\n",
    "    # --- Read preference from state ---\n",
    "    preferred_unit = tool_context.state.get(\"user_preference_temperature_unit\", \"Celsius\") # Default to Celsius\n",
    "    print(f\"--- Tool: Reading state 'user_preference_temperature_unit': {preferred_unit} ---\")\n",
    "\n",
    "    city_normalized = city.lower().replace(\" \", \"\")\n",
    "\n",
    "    # Mock weather data (always stored in Celsius internally)\n",
    "    mock_weather_db = {\n",
    "        \"newyork\": {\"temp_c\": 25, \"condition\": \"sunny\"},\n",
    "        \"london\": {\"temp_c\": 15, \"condition\": \"cloudy\"},\n",
    "        \"tokyo\": {\"temp_c\": 18, \"condition\": \"light rain\"},\n",
    "    }\n",
    "\n",
    "    if city_normalized in mock_weather_db:\n",
    "        data = mock_weather_db[city_normalized]\n",
    "        temp_c = data[\"temp_c\"]\n",
    "        condition = data[\"condition\"]\n",
    "\n",
    "        # Format temperature based on state preference\n",
    "        if preferred_unit == \"Fahrenheit\":\n",
    "            temp_value = (temp_c * 9/5) + 32 # Calculate Fahrenheit\n",
    "            temp_unit = \"°F\"\n",
    "        else: # Default to Celsius\n",
    "            temp_value = temp_c\n",
    "            temp_unit = \"°C\"\n",
    "\n",
    "        report = f\"The weather in {city.capitalize()} is {condition} with a temperature of {temp_value:.0f}{temp_unit}.\"\n",
    "        result = {\"status\": \"success\", \"report\": report}\n",
    "        print(f\"--- Tool: Generated report in {preferred_unit}. Result: {result} ---\")\n",
    "\n",
    "        # Example of writing back to state (optional for this tool)\n",
    "        tool_context.state[\"last_city_checked_stateful\"] = city\n",
    "        print(f\"--- Tool: Updated state 'last_city_checked_stateful': {city} ---\")\n",
    "\n",
    "        return result\n",
    "    else:\n",
    "        # Handle city not found\n",
    "        error_msg = f\"Sorry, I don't have weather information for '{city}'.\"\n",
    "        print(f\"--- Tool: City '{city}' not found. ---\")\n",
    "        return {\"status\": \"error\", \"error_message\": error_msg}\n",
    "\n",
    "print(\"✅ State-aware 'get_weather_stateful' tool defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65224a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Agent 'greeting_agent' redefined.\n",
      "✅ Agent 'farewell_agent' redefined.\n",
      "✅ Root Agent 'weather_agent_v4_stateful' created using stateful tool and output_key.\n",
      "✅ Runner created for stateful root agent 'weather_agent_v4_stateful' using stateful session service.\n"
     ]
    }
   ],
   "source": [
    "# @title 3. Redefine Sub-Agents and Update Root Agent with output_key\n",
    "\n",
    "# Ensure necessary imports: Agent, LiteLlm, Runner\n",
    "from google.adk.agents import Agent\n",
    "from google.adk.models.lite_llm import LiteLlm\n",
    "from google.adk.runners import Runner\n",
    "# Ensure tools 'say_hello', 'say_goodbye' are defined (from Step 3)\n",
    "# Ensure model constants MODEL_GPT_4O, MODEL_GEMINI_2_0_FLASH etc. are defined\n",
    "\n",
    "# --- Redefine Greeting Agent (from Step 3) ---\n",
    "greeting_agent = None\n",
    "try:\n",
    "    greeting_agent = Agent(\n",
    "        model=MODEL_GEMINI_2_0_FLASH,\n",
    "        name=\"greeting_agent\",\n",
    "        instruction=\"You are the Greeting Agent. Your ONLY task is to provide a friendly greeting using the 'say_hello' tool. Do nothing else.\",\n",
    "        description=\"Handles simple greetings and hellos using the 'say_hello' tool.\",\n",
    "        tools=[say_hello],\n",
    "    )\n",
    "    print(f\"✅ Agent '{greeting_agent.name}' redefined.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Could not redefine Greeting agent. Error: {e}\")\n",
    "\n",
    "# --- Redefine Farewell Agent (from Step 3) ---\n",
    "farewell_agent = None\n",
    "try:\n",
    "    farewell_agent = Agent(\n",
    "        model=MODEL_GEMINI_2_0_FLASH,\n",
    "        name=\"farewell_agent\",\n",
    "        instruction=\"You are the Farewell Agent. Your ONLY task is to provide a polite goodbye message using the 'say_goodbye' tool. Do not perform any other actions.\",\n",
    "        description=\"Handles simple farewells and goodbyes using the 'say_goodbye' tool.\",\n",
    "        tools=[say_goodbye],\n",
    "    )\n",
    "    print(f\"✅ Agent '{farewell_agent.name}' redefined.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Could not redefine Farewell agent. Error: {e}\")\n",
    "\n",
    "# --- Define the Updated Root Agent ---\n",
    "root_agent_stateful = None\n",
    "runner_root_stateful = None # Initialize runner\n",
    "\n",
    "# Check prerequisites before creating the root agent\n",
    "if greeting_agent and farewell_agent and 'get_weather_stateful' in globals():\n",
    "\n",
    "    root_agent_model = MODEL_GEMINI_2_0_FLASH # Choose orchestration model\n",
    "\n",
    "    root_agent_stateful = Agent(\n",
    "        name=\"weather_agent_v4_stateful\", # New version name\n",
    "        model=root_agent_model,\n",
    "        description=\"Main agent: Provides weather (state-aware unit), delegates greetings/farewells, saves report to state.\",\n",
    "        instruction=\"You are the main Weather Agent. Your job is to provide weather using 'get_weather_stateful'. \"\n",
    "                    \"The tool will format the temperature based on user preference stored in state. \"\n",
    "                    \"Delegate simple greetings to 'greeting_agent' and farewells to 'farewell_agent'. \"\n",
    "                    \"Handle only weather requests, greetings, and farewells.\",\n",
    "        tools=[get_weather_stateful], # Use the state-aware tool\n",
    "        sub_agents=[greeting_agent, farewell_agent], # Include sub-agents\n",
    "        output_key=\"last_weather_report\" # <<< Auto-save agent's final weather response\n",
    "    )\n",
    "    print(f\"✅ Root Agent '{root_agent_stateful.name}' created using stateful tool and output_key.\")\n",
    "\n",
    "    # --- Create Runner for this Root Agent & NEW Session Service ---\n",
    "    runner_root_stateful = Runner(\n",
    "        agent=root_agent_stateful,\n",
    "        app_name=APP_NAME,\n",
    "        session_service=session_service_stateful # Use the NEW stateful session service\n",
    "    )\n",
    "    print(f\"✅ Runner created for stateful root agent '{runner_root_stateful.agent.name}' using stateful session service.\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ Cannot create stateful root agent. Prerequisites missing.\")\n",
    "    if not greeting_agent: print(\" - greeting_agent definition missing.\")\n",
    "    if not farewell_agent: print(\" - farewell_agent definition missing.\")\n",
    "    if 'get_weather_stateful' not in globals(): print(\" - get_weather_stateful tool missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdad913",
   "metadata": {},
   "source": [
    "No I dont think this memory is similar to Langmem - this is just maintaining a state which is a dictionary - similar to initial_state in the graph - they are using the state to maintain the preferences of the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4afd117c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting execution using 'await' (default for notebooks)...\n",
      "\n",
      "--- Testing State: Temp Unit Conversion & output_key ---\n",
      "--- Turn 1: Requesting weather in London (expect Celsius) ---\n",
      "\n",
      ">>> User Query: What's the weather in London?\n",
      "--- Tool: get_weather_stateful called for London ---\n",
      "--- Tool: Reading state 'user_preference_temperature_unit': Fahrenheit ---\n",
      "--- Tool: Generated report in Fahrenheit. Result: {'status': 'success', 'report': 'The weather in London is cloudy with a temperature of 59°F.'} ---\n",
      "--- Tool: Updated state 'last_city_checked_stateful': London ---\n",
      "<<< Agent Response: The weather in London is cloudy with a temperature of 59°F.\n",
      "\n",
      "\n",
      "--- Manually Updating State: Setting unit to Fahrenheit ---\n",
      "--- Stored session state updated. Current 'user_preference_temperature_unit': Fahrenheit ---\n",
      "\n",
      "--- Turn 2: Requesting weather in New York (expect Fahrenheit) ---\n",
      "\n",
      ">>> User Query: Tell me the weather in New York.\n",
      "--- Tool: get_weather_stateful called for New York ---\n",
      "--- Tool: Reading state 'user_preference_temperature_unit': Fahrenheit ---\n",
      "--- Tool: Generated report in Fahrenheit. Result: {'status': 'success', 'report': 'The weather in New york is sunny with a temperature of 77°F.'} ---\n",
      "--- Tool: Updated state 'last_city_checked_stateful': New York ---\n",
      "<<< Agent Response: The weather in New york is sunny with a temperature of 77°F.\n",
      "\n",
      "\n",
      "--- Turn 3: Sending a greeting ---\n",
      "\n",
      ">>> User Query: Hi!\n",
      "<<< Agent Response: Hello!\n",
      "\n",
      "\n",
      "--- Inspecting Final Session State ---\n",
      "Final Preference: Fahrenheit\n",
      "Final Last Weather Report (from output_key): The weather in New york is sunny with a temperature of 77°F.\n",
      "\n",
      "Final Last City Checked (by tool): New York\n"
     ]
    }
   ],
   "source": [
    "# @title 4. Interact to Test State Flow and output_key\n",
    "import asyncio # Ensure asyncio is imported\n",
    "\n",
    "# Ensure the stateful runner (runner_root_stateful) is available from the previous cell\n",
    "# Ensure call_agent_async, USER_ID_STATEFUL, SESSION_ID_STATEFUL, APP_NAME are defined\n",
    "\n",
    "if 'runner_root_stateful' in globals() and runner_root_stateful:\n",
    "    # Define the main async function for the stateful conversation logic.\n",
    "    # The 'await' keywords INSIDE this function are necessary for async operations.\n",
    "    async def run_stateful_conversation():\n",
    "        print(\"\\n--- Testing State: Temp Unit Conversion & output_key ---\")\n",
    "\n",
    "        # 1. Check weather (Uses initial state: Celsius)\n",
    "        print(\"--- Turn 1: Requesting weather in London (expect Celsius) ---\")\n",
    "        await call_agent_async(query= \"What's the weather in London?\",\n",
    "                               runner=runner_root_stateful,\n",
    "                               user_id=USER_ID_STATEFUL,\n",
    "                               session_id=SESSION_ID_STATEFUL\n",
    "                              )\n",
    "\n",
    "        # 2. Manually update state preference to Fahrenheit - DIRECTLY MODIFY STORAGE\n",
    "        print(\"\\n--- Manually Updating State: Setting unit to Fahrenheit ---\")\n",
    "        try:\n",
    "            # Access the internal storage directly - THIS IS SPECIFIC TO InMemorySessionService for testing\n",
    "            # NOTE: In production with persistent services (Database, VertexAI), you would\n",
    "            # typically update state via agent actions or specific service APIs if available,\n",
    "            # not by direct manipulation of internal storage.\n",
    "            stored_session = session_service_stateful.sessions[APP_NAME][USER_ID_STATEFUL][SESSION_ID_STATEFUL]\n",
    "            stored_session.state[\"user_preference_temperature_unit\"] = \"Fahrenheit\"\n",
    "            # Optional: You might want to update the timestamp as well if any logic depends on it\n",
    "            # import time\n",
    "            # stored_session.last_update_time = time.time()\n",
    "            print(f\"--- Stored session state updated. Current 'user_preference_temperature_unit': {stored_session.state.get('user_preference_temperature_unit', 'Not Set')} ---\") # Added .get for safety\n",
    "        except KeyError:\n",
    "            print(f\"--- Error: Could not retrieve session '{SESSION_ID_STATEFUL}' from internal storage for user '{USER_ID_STATEFUL}' in app '{APP_NAME}' to update state. Check IDs and if session was created. ---\")\n",
    "        except Exception as e:\n",
    "             print(f\"--- Error updating internal session state: {e} ---\")\n",
    "\n",
    "        # 3. Check weather again (Tool should now use Fahrenheit)\n",
    "        # This will also update 'last_weather_report' via output_key\n",
    "        print(\"\\n--- Turn 2: Requesting weather in New York (expect Fahrenheit) ---\")\n",
    "        await call_agent_async(query= \"Tell me the weather in New York.\",\n",
    "                               runner=runner_root_stateful,\n",
    "                               user_id=USER_ID_STATEFUL,\n",
    "                               session_id=SESSION_ID_STATEFUL\n",
    "                              )\n",
    "\n",
    "        # 4. Test basic delegation (should still work)\n",
    "        # This will update 'last_weather_report' again, overwriting the NY weather report\n",
    "        print(\"\\n--- Turn 3: Sending a greeting ---\")\n",
    "        await call_agent_async(query= \"Hi!\",\n",
    "                               runner=runner_root_stateful,\n",
    "                               user_id=USER_ID_STATEFUL,\n",
    "                               session_id=SESSION_ID_STATEFUL\n",
    "                              )\n",
    "\n",
    "    # --- Execute the `run_stateful_conversation` async function ---\n",
    "    # Choose ONE of the methods below based on your environment.\n",
    "\n",
    "    # METHOD 1: Direct await (Default for Notebooks/Async REPLs)\n",
    "    # If your environment supports top-level await (like Colab/Jupyter notebooks),\n",
    "    # it means an event loop is already running, so you can directly await the function.\n",
    "    print(\"Attempting execution using 'await' (default for notebooks)...\")\n",
    "    await run_stateful_conversation()\n",
    "\n",
    "    # METHOD 2: asyncio.run (For Standard Python Scripts [.py])\n",
    "    # If running this code as a standard Python script from your terminal,\n",
    "    # the script context is synchronous. `asyncio.run()` is needed to\n",
    "    # create and manage an event loop to execute your async function.\n",
    "    # To use this method:\n",
    "    # 1. Comment out the `await run_stateful_conversation()` line above.\n",
    "    # 2. Uncomment the following block:\n",
    "    \"\"\"\n",
    "    import asyncio\n",
    "    if __name__ == \"__main__\": # Ensures this runs only when script is executed directly\n",
    "        print(\"Executing using 'asyncio.run()' (for standard Python scripts)...\")\n",
    "        try:\n",
    "            # This creates an event loop, runs your async function, and closes the loop.\n",
    "            asyncio.run(run_stateful_conversation())\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Inspect final session state after the conversation ---\n",
    "    # This block runs after either execution method completes.\n",
    "    print(\"\\n--- Inspecting Final Session State ---\")\n",
    "    final_session = await session_service_stateful.get_session(app_name=APP_NAME,\n",
    "                                                         user_id= USER_ID_STATEFUL,\n",
    "                                                         session_id=SESSION_ID_STATEFUL)\n",
    "    if final_session:\n",
    "        # Use .get() for safer access to potentially missing keys\n",
    "        print(f\"Final Preference: {final_session.state.get('user_preference_temperature_unit', 'Not Set')}\")\n",
    "        print(f\"Final Last Weather Report (from output_key): {final_session.state.get('last_weather_report', 'Not Set')}\")\n",
    "        print(f\"Final Last City Checked (by tool): {final_session.state.get('last_city_checked_stateful', 'Not Set')}\")\n",
    "        # Print full state for detailed view\n",
    "        # print(f\"Full State Dict: {final_session.state}\") # For detailed view\n",
    "    else:\n",
    "        print(\"\\n❌ Error: Could not retrieve final session state.\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n⚠️ Skipping state test conversation. Stateful root agent runner ('runner_root_stateful') is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f2c034",
   "metadata": {},
   "source": [
    "One major advantage of agent adk that I find is we can add new keys to the state during agent execution. I don't think this is possible in LangGraph - should check it out. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3270b7c0",
   "metadata": {},
   "source": [
    "## Adding Safety - Input Guardrail with before_model_callback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbe6b92",
   "metadata": {},
   "source": [
    "What is before_model_callback?\n",
    "\n",
    "It's a Python function you define that ADK executes just before an agent sends its compiled request (including conversation history, instructions, and the latest user message) to the underlying LLM.\n",
    "Purpose: Inspect the request, modify it if necessary, or block it entirely based on predefined rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300e5221",
   "metadata": {},
   "source": [
    "Common Use Cases:\n",
    "\n",
    "    Input Validation/Filtering: Check if user input meets criteria or contains disallowed content (like PII or keywords).\n",
    "\n",
    "    Guardrails: Prevent harmful, off-topic, or policy-violating requests from being processed by the LLM.\n",
    "\n",
    "    Dynamic Prompt Modification: Add timely information (e.g., from session state) to the LLM request context just before sending."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cd7f98",
   "metadata": {},
   "source": [
    "How it Works:\n",
    "\n",
    "Define a function accepting callback_context: CallbackContext and llm_request: LlmRequest.\n",
    "\n",
    "    callback_context: Provides access to agent info, session state (callback_context.state), etc.\n",
    "\n",
    "    llm_request: Contains the full payload intended for the LLM (contents, config).\n",
    "\n",
    "Inside the function:\n",
    "\n",
    "    Inspect: Examine llm_request.contents (especially the last user message).\n",
    "\n",
    "    Modify (Use Caution): You can change parts of llm_request.\n",
    "\n",
    "    Block (Guardrail): Return an LlmResponse object. ADK will send this response back immediately, skipping the LLM call for that turn.\n",
    "    \n",
    "    Allow: Return None. ADK proceeds to call the LLM with the (potentially modified) request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "50ca70f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ block_keyword_guardrail function defined.\n"
     ]
    }
   ],
   "source": [
    "# @title 1. Define the before_model_callback Guardrail\n",
    "\n",
    "# Ensure necessary imports are available\n",
    "from google.adk.agents.callback_context import CallbackContext\n",
    "from google.adk.models.llm_request import LlmRequest\n",
    "from google.adk.models.llm_response import LlmResponse\n",
    "from google.genai import types # For creating response content\n",
    "from typing import Optional\n",
    "\n",
    "def block_keyword_guardrail(\n",
    "    callback_context: CallbackContext, llm_request: LlmRequest\n",
    ") -> Optional[LlmResponse]:\n",
    "    \"\"\"\n",
    "    Inspects the latest user message for 'BLOCK'. If found, blocks the LLM call\n",
    "    and returns a predefined LlmResponse. Otherwise, returns None to proceed.\n",
    "    \"\"\"\n",
    "    agent_name = callback_context.agent_name # Get the name of the agent whose model call is being intercepted\n",
    "    print(f\"--- Callback: block_keyword_guardrail running for agent: {agent_name} ---\")\n",
    "\n",
    "    # Extract the text from the latest user message in the request history\n",
    "    last_user_message_text = \"\"\n",
    "    if llm_request.contents:\n",
    "        # Find the most recent message with role 'user'\n",
    "        for content in reversed(llm_request.contents):\n",
    "            if content.role == 'user' and content.parts:\n",
    "                # Assuming text is in the first part for simplicity\n",
    "                if content.parts[0].text:\n",
    "                    last_user_message_text = content.parts[0].text\n",
    "                    break # Found the last user message text\n",
    "\n",
    "    print(f\"--- Callback: Inspecting last user message: '{last_user_message_text[:100]}...' ---\") # Log first 100 chars\n",
    "\n",
    "    # --- Guardrail Logic ---\n",
    "    keyword_to_block = \"BLOCK\"\n",
    "    if keyword_to_block in last_user_message_text.upper(): # Case-insensitive check\n",
    "        print(f\"--- Callback: Found '{keyword_to_block}'. Blocking LLM call! ---\")\n",
    "        # Optionally, set a flag in state to record the block event\n",
    "        callback_context.state[\"guardrail_block_keyword_triggered\"] = True\n",
    "        print(f\"--- Callback: Set state 'guardrail_block_keyword_triggered': True ---\")\n",
    "\n",
    "        # Construct and return an LlmResponse to stop the flow and send this back instead\n",
    "        return LlmResponse(\n",
    "            content=types.Content(\n",
    "                role=\"model\", # Mimic a response from the agent's perspective\n",
    "                parts=[types.Part(text=f\"I cannot process this request because it contains the blocked keyword '{keyword_to_block}'.\")],\n",
    "            )\n",
    "            # Note: You could also set an error_message field here if needed\n",
    "        )\n",
    "    else:\n",
    "        # Keyword not found, allow the request to proceed to the LLM\n",
    "        print(f\"--- Callback: Keyword not found. Allowing LLM call for {agent_name}. ---\")\n",
    "        return None # Returning None signals ADK to continue normally\n",
    "\n",
    "print(\"✅ block_keyword_guardrail function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "873d95e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sub-Agent 'greeting_agent' redefined.\n",
      "✅ Sub-Agent 'farewell_agent' redefined.\n",
      "✅ Root Agent 'weather_agent_v5_model_guardrail' created with before_model_callback.\n",
      "✅ Runner created for guardrail agent 'weather_agent_v5_model_guardrail', using stateful session service.\n"
     ]
    }
   ],
   "source": [
    "# @title 2. Update Root Agent with before_model_callback\n",
    "\n",
    "\n",
    "# --- Redefine Sub-Agents (Ensures they exist in this context) ---\n",
    "greeting_agent = None\n",
    "try:\n",
    "    # Use a defined model constant\n",
    "    greeting_agent = Agent(\n",
    "        model=MODEL_GEMINI_2_0_FLASH,\n",
    "        name=\"greeting_agent\", # Keep original name for consistency\n",
    "        instruction=\"You are the Greeting Agent. Your ONLY task is to provide a friendly greeting using the 'say_hello' tool. Do nothing else.\",\n",
    "        description=\"Handles simple greetings and hellos using the 'say_hello' tool.\",\n",
    "        tools=[say_hello],\n",
    "    )\n",
    "    print(f\"✅ Sub-Agent '{greeting_agent.name}' redefined.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Could not redefine Greeting agent. Check Model/API Key ({greeting_agent.model}). Error: {e}\")\n",
    "\n",
    "farewell_agent = None\n",
    "try:\n",
    "    # Use a defined model constant\n",
    "    farewell_agent = Agent(\n",
    "        model=MODEL_GEMINI_2_0_FLASH,\n",
    "        name=\"farewell_agent\", # Keep original name\n",
    "        instruction=\"You are the Farewell Agent. Your ONLY task is to provide a polite goodbye message using the 'say_goodbye' tool. Do not perform any other actions.\",\n",
    "        description=\"Handles simple farewells and goodbyes using the 'say_goodbye' tool.\",\n",
    "        tools=[say_goodbye],\n",
    "    )\n",
    "    print(f\"✅ Sub-Agent '{farewell_agent.name}' redefined.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Could not redefine Farewell agent. Check Model/API Key ({farewell_agent.model}). Error: {e}\")\n",
    "\n",
    "\n",
    "# --- Define the Root Agent with the Callback ---\n",
    "root_agent_model_guardrail = None\n",
    "runner_root_model_guardrail = None\n",
    "\n",
    "# Check all components before proceeding\n",
    "if greeting_agent and farewell_agent and 'get_weather_stateful' in globals() and 'block_keyword_guardrail' in globals():\n",
    "\n",
    "    # Use a defined model constant\n",
    "    root_agent_model = MODEL_GEMINI_2_0_FLASH\n",
    "\n",
    "    root_agent_model_guardrail = Agent(\n",
    "        name=\"weather_agent_v5_model_guardrail\", # New version name for clarity\n",
    "        model=root_agent_model,\n",
    "        description=\"Main agent: Handles weather, delegates greetings/farewells, includes input keyword guardrail.\",\n",
    "        instruction=\"You are the main Weather Agent. Provide weather using 'get_weather_stateful'. \"\n",
    "                    \"Delegate simple greetings to 'greeting_agent' and farewells to 'farewell_agent'. \"\n",
    "                    \"Handle only weather requests, greetings, and farewells.\",\n",
    "        tools=[get_weather_stateful],\n",
    "        sub_agents=[greeting_agent, farewell_agent], # Reference the redefined sub-agents\n",
    "        output_key=\"last_weather_report\", # Keep output_key from Step 4\n",
    "        before_model_callback=block_keyword_guardrail # <<< Assign the guardrail callback\n",
    "    )\n",
    "    print(f\"✅ Root Agent '{root_agent_model_guardrail.name}' created with before_model_callback.\")\n",
    "\n",
    "    # --- Create Runner for this Agent, Using SAME Stateful Session Service ---\n",
    "    # Ensure session_service_stateful exists from Step 4\n",
    "    if 'session_service_stateful' in globals():\n",
    "        runner_root_model_guardrail = Runner(\n",
    "            agent=root_agent_model_guardrail,\n",
    "            app_name=APP_NAME, # Use consistent APP_NAME\n",
    "            session_service=session_service_stateful # <<< Use the service from Step 4\n",
    "        )\n",
    "        print(f\"✅ Runner created for guardrail agent '{runner_root_model_guardrail.agent.name}', using stateful session service.\")\n",
    "    else:\n",
    "        print(\"❌ Cannot create runner. 'session_service_stateful' from Step 4 is missing.\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ Cannot create root agent with model guardrail. One or more prerequisites are missing or failed initialization:\")\n",
    "    if not greeting_agent: print(\"   - Greeting Agent\")\n",
    "    if not farewell_agent: print(\"   - Farewell Agent\")\n",
    "    if 'get_weather_stateful' not in globals(): print(\"   - 'get_weather_stateful' tool\")\n",
    "    if 'block_keyword_guardrail' not in globals(): print(\"   - 'block_keyword_guardrail' callback\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6bf432f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting execution using 'await' (default for notebooks)...\n",
      "\n",
      "--- Testing Model Input Guardrail ---\n",
      "--- Turn 1: Requesting weather in London (expect allowed, Fahrenheit) ---\n",
      "\n",
      ">>> User Query: What is the weather in London?\n",
      "--- Callback: block_keyword_guardrail running for agent: weather_agent_v5_model_guardrail ---\n",
      "--- Callback: Inspecting last user message: 'For context:...' ---\n",
      "--- Callback: Keyword not found. Allowing LLM call for weather_agent_v5_model_guardrail. ---\n",
      "--- Tool: get_weather_stateful called for London ---\n",
      "--- Tool: Reading state 'user_preference_temperature_unit': Fahrenheit ---\n",
      "--- Tool: Generated report in Fahrenheit. Result: {'status': 'success', 'report': 'The weather in London is cloudy with a temperature of 59°F.'} ---\n",
      "--- Tool: Updated state 'last_city_checked_stateful': London ---\n",
      "--- Callback: block_keyword_guardrail running for agent: weather_agent_v5_model_guardrail ---\n",
      "--- Callback: Inspecting last user message: 'For context:...' ---\n",
      "--- Callback: Keyword not found. Allowing LLM call for weather_agent_v5_model_guardrail. ---\n",
      "<<< Agent Response: The weather in London is cloudy with a temperature of 59°F.\n",
      "\n",
      "\n",
      "--- Turn 2: Requesting with blocked keyword (expect blocked) ---\n",
      "\n",
      ">>> User Query: BLOCK the request for weather in Tokyo\n",
      "--- Callback: block_keyword_guardrail running for agent: weather_agent_v5_model_guardrail ---\n",
      "--- Callback: Inspecting last user message: 'BLOCK the request for weather in Tokyo...' ---\n",
      "--- Callback: Found 'BLOCK'. Blocking LLM call! ---\n",
      "--- Callback: Set state 'guardrail_block_keyword_triggered': True ---\n",
      "<<< Agent Response: I cannot process this request because it contains the blocked keyword 'BLOCK'.\n",
      "\n",
      "--- Turn 3: Sending a greeting (expect allowed) ---\n",
      "\n",
      ">>> User Query: Hello again\n",
      "--- Callback: block_keyword_guardrail running for agent: weather_agent_v5_model_guardrail ---\n",
      "--- Callback: Inspecting last user message: 'Hello again...' ---\n",
      "--- Callback: Keyword not found. Allowing LLM call for weather_agent_v5_model_guardrail. ---\n",
      "<<< Agent Response: Hello!\n",
      "\n",
      "\n",
      "--- Inspecting Final Session State (After Guardrail Test) ---\n",
      "Guardrail Triggered Flag: True\n",
      "Last Weather Report: I cannot process this request because it contains the blocked keyword 'BLOCK'.\n",
      "Temperature Unit: Fahrenheit\n"
     ]
    }
   ],
   "source": [
    "# @title 3. Interact to Test the Model Input Guardrail\n",
    "import asyncio # Ensure asyncio is imported\n",
    "\n",
    "# Ensure the runner for the guardrail agent is available\n",
    "if 'runner_root_model_guardrail' in globals() and runner_root_model_guardrail:\n",
    "    # Define the main async function for the guardrail test conversation.\n",
    "    # The 'await' keywords INSIDE this function are necessary for async operations.\n",
    "    async def run_guardrail_test_conversation():\n",
    "        print(\"\\n--- Testing Model Input Guardrail ---\")\n",
    "\n",
    "        # Use the runner for the agent with the callback and the existing stateful session ID\n",
    "        # Define a helper lambda for cleaner interaction calls\n",
    "        interaction_func = lambda query: call_agent_async(query,\n",
    "                                                         runner_root_model_guardrail,\n",
    "                                                         USER_ID_STATEFUL, # Use existing user ID\n",
    "                                                         SESSION_ID_STATEFUL # Use existing session ID\n",
    "                                                        )\n",
    "        # 1. Normal request (Callback allows, should use Fahrenheit from previous state change)\n",
    "        print(\"--- Turn 1: Requesting weather in London (expect allowed, Fahrenheit) ---\")\n",
    "        await interaction_func(\"What is the weather in London?\")\n",
    "\n",
    "        # 2. Request containing the blocked keyword (Callback intercepts)\n",
    "        print(\"\\n--- Turn 2: Requesting with blocked keyword (expect blocked) ---\")\n",
    "        await interaction_func(\"BLOCK the request for weather in Tokyo\") # Callback should catch \"BLOCK\"\n",
    "\n",
    "        # 3. Normal greeting (Callback allows root agent, delegation happens)\n",
    "        print(\"\\n--- Turn 3: Sending a greeting (expect allowed) ---\")\n",
    "        await interaction_func(\"Hello again\")\n",
    "\n",
    "    # --- Execute the `run_guardrail_test_conversation` async function ---\n",
    "    # Choose ONE of the methods below based on your environment.\n",
    "\n",
    "    # METHOD 1: Direct await (Default for Notebooks/Async REPLs)\n",
    "    # If your environment supports top-level await (like Colab/Jupyter notebooks),\n",
    "    # it means an event loop is already running, so you can directly await the function.\n",
    "    print(\"Attempting execution using 'await' (default for notebooks)...\")\n",
    "    await run_guardrail_test_conversation()\n",
    "\n",
    "    # METHOD 2: asyncio.run (For Standard Python Scripts [.py])\n",
    "    # If running this code as a standard Python script from your terminal,\n",
    "    # the script context is synchronous. `asyncio.run()` is needed to\n",
    "    # create and manage an event loop to execute your async function.\n",
    "    # To use this method:\n",
    "    # 1. Comment out the `await run_guardrail_test_conversation()` line above.\n",
    "    # 2. Uncomment the following block:\n",
    "    \"\"\"\n",
    "    import asyncio\n",
    "    if __name__ == \"__main__\": # Ensures this runs only when script is executed directly\n",
    "        print(\"Executing using 'asyncio.run()' (for standard Python scripts)...\")\n",
    "        try:\n",
    "            # This creates an event loop, runs your async function, and closes the loop.\n",
    "            asyncio.run(run_guardrail_test_conversation())\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Inspect final session state after the conversation ---\n",
    "    # This block runs after either execution method completes.\n",
    "    # Optional: Check state for the trigger flag set by the callback\n",
    "    print(\"\\n--- Inspecting Final Session State (After Guardrail Test) ---\")\n",
    "    # Use the session service instance associated with this stateful session\n",
    "    final_session = await session_service_stateful.get_session(app_name=APP_NAME,\n",
    "                                                         user_id=USER_ID_STATEFUL,\n",
    "                                                         session_id=SESSION_ID_STATEFUL)\n",
    "    if final_session:\n",
    "        # Use .get() for safer access\n",
    "        print(f\"Guardrail Triggered Flag: {final_session.state.get('guardrail_block_keyword_triggered', 'Not Set (or False)')}\")\n",
    "        print(f\"Last Weather Report: {final_session.state.get('last_weather_report', 'Not Set')}\") # Should be London weather if successful\n",
    "        print(f\"Temperature Unit: {final_session.state.get('user_preference_temperature_unit', 'Not Set')}\") # Should be Fahrenheit\n",
    "        # print(f\"Full State Dict: {final_session.state}\") # For detailed view\n",
    "    else:\n",
    "        print(\"\\n❌ Error: Could not retrieve final session state.\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n⚠️ Skipping model guardrail test. Runner ('runner_root_model_guardrail') is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f66168",
   "metadata": {},
   "source": [
    "## Adding Safety - Tool Argument Guardrail (before_tool_callback)\n",
    "\n",
    "This is a new guardrail that I am seeing - neven thought of imposing a guardrail to a tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7939e1",
   "metadata": {},
   "source": [
    "What is before_tool_callback?\n",
    "\n",
    "    It's a Python function executed just before a specific tool function runs, after the LLM has requested its use and decided on the arguments.\n",
    "\n",
    "    Purpose: Validate tool arguments, prevent tool execution based on specific inputs, modify arguments dynamically, or enforce resource usage policies.\n",
    "\n",
    "Common Use Cases:\n",
    "\n",
    "    Argument Validation: Check if arguments provided by the LLM are valid, within allowed ranges, or conform to expected formats.\n",
    "\n",
    "    Resource Protection: Prevent tools from being called with inputs that might be costly, access restricted data, or cause unwanted side effects (e.g., blocking API calls for certain parameters).\n",
    "\n",
    "    Dynamic Argument Modification: Adjust arguments based on session state or other contextual information before the tool runs.\n",
    "\n",
    "See if there is a after_tool_callback as will so that you can maybe do Agentic RAG easily and filter out only necessary things\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f75358fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ block_paris_tool_guardrail function defined.\n"
     ]
    }
   ],
   "source": [
    "# @title 1. Define the before_tool_callback Guardrail\n",
    "\n",
    "# Ensure necessary imports are available\n",
    "from google.adk.tools.base_tool import BaseTool\n",
    "from google.adk.tools.tool_context import ToolContext\n",
    "from typing import Optional, Dict, Any # For type hints\n",
    "\n",
    "def block_paris_tool_guardrail(\n",
    "    tool: BaseTool, args: Dict[str, Any], tool_context: ToolContext\n",
    ") -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Checks if 'get_weather_stateful' is called for 'Paris'.\n",
    "    If so, blocks the tool execution and returns a specific error dictionary.\n",
    "    Otherwise, allows the tool call to proceed by returning None.\n",
    "    \"\"\"\n",
    "    tool_name = tool.name\n",
    "    agent_name = tool_context.agent_name # Agent attempting the tool call\n",
    "    print(f\"--- Callback: block_paris_tool_guardrail running for tool '{tool_name}' in agent '{agent_name}' ---\")\n",
    "    print(f\"--- Callback: Inspecting args: {args} ---\")\n",
    "\n",
    "    # --- Guardrail Logic ---\n",
    "    target_tool_name = \"get_weather_stateful\" # Match the function name used by FunctionTool\n",
    "    blocked_city = \"paris\"\n",
    "\n",
    "    # Check if it's the correct tool and the city argument matches the blocked city\n",
    "    if tool_name == target_tool_name:\n",
    "        city_argument = args.get(\"city\", \"\") # Safely get the 'city' argument\n",
    "        if city_argument and city_argument.lower() == blocked_city:\n",
    "            print(f\"--- Callback: Detected blocked city '{city_argument}'. Blocking tool execution! ---\")\n",
    "            # Optionally update state\n",
    "            tool_context.state[\"guardrail_tool_block_triggered\"] = True\n",
    "            print(f\"--- Callback: Set state 'guardrail_tool_block_triggered': True ---\")\n",
    "\n",
    "            # Return a dictionary matching the tool's expected output format for errors\n",
    "            # This dictionary becomes the tool's result, skipping the actual tool run.\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"error_message\": f\"Policy restriction: Weather checks for '{city_argument.capitalize()}' are currently disabled by a tool guardrail.\"\n",
    "            }\n",
    "        else:\n",
    "             print(f\"--- Callback: City '{city_argument}' is allowed for tool '{tool_name}'. ---\")\n",
    "    else:\n",
    "        print(f\"--- Callback: Tool '{tool_name}' is not the target tool. Allowing. ---\")\n",
    "\n",
    "\n",
    "    # If the checks above didn't return a dictionary, allow the tool to execute\n",
    "    print(f\"--- Callback: Allowing tool '{tool_name}' to proceed. ---\")\n",
    "    return None # Returning None allows the actual tool function to run\n",
    "\n",
    "print(\"✅ block_paris_tool_guardrail function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7811960f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sub-Agent 'greeting_agent' redefined.\n",
      "✅ Sub-Agent 'farewell_agent' redefined.\n",
      "✅ Root Agent 'weather_agent_v6_tool_guardrail' created with BOTH callbacks.\n",
      "✅ Runner created for tool guardrail agent 'weather_agent_v6_tool_guardrail', using stateful session service.\n"
     ]
    }
   ],
   "source": [
    "# @title 2. Update Root Agent with BOTH Callbacks (Self-Contained)\n",
    "\n",
    "# --- Ensure Prerequisites are Defined ---\n",
    "# (Include or ensure execution of definitions for: Agent, LiteLlm, Runner, ToolContext,\n",
    "#  MODEL constants, say_hello, say_goodbye, greeting_agent, farewell_agent,\n",
    "#  get_weather_stateful, block_keyword_guardrail, block_paris_tool_guardrail)\n",
    "\n",
    "# --- Redefine Sub-Agents (Ensures they exist in this context) ---\n",
    "greeting_agent = None\n",
    "try:\n",
    "    # Use a defined model constant\n",
    "    greeting_agent = Agent(\n",
    "        model=MODEL_GEMINI_2_0_FLASH,\n",
    "        name=\"greeting_agent\", # Keep original name for consistency\n",
    "        instruction=\"You are the Greeting Agent. Your ONLY task is to provide a friendly greeting using the 'say_hello' tool. Do nothing else.\",\n",
    "        description=\"Handles simple greetings and hellos using the 'say_hello' tool.\",\n",
    "        tools=[say_hello],\n",
    "    )\n",
    "    print(f\"✅ Sub-Agent '{greeting_agent.name}' redefined.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Could not redefine Greeting agent. Check Model/API Key ({greeting_agent.model}). Error: {e}\")\n",
    "\n",
    "farewell_agent = None\n",
    "try:\n",
    "    # Use a defined model constant\n",
    "    farewell_agent = Agent(\n",
    "        model=MODEL_GEMINI_2_0_FLASH,\n",
    "        name=\"farewell_agent\", # Keep original name\n",
    "        instruction=\"You are the Farewell Agent. Your ONLY task is to provide a polite goodbye message using the 'say_goodbye' tool. Do not perform any other actions.\",\n",
    "        description=\"Handles simple farewells and goodbyes using the 'say_goodbye' tool.\",\n",
    "        tools=[say_goodbye],\n",
    "    )\n",
    "    print(f\"✅ Sub-Agent '{farewell_agent.name}' redefined.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Could not redefine Farewell agent. Check Model/API Key ({farewell_agent.model}). Error: {e}\")\n",
    "\n",
    "# --- Define the Root Agent with Both Callbacks ---\n",
    "root_agent_tool_guardrail = None\n",
    "runner_root_tool_guardrail = None\n",
    "\n",
    "if ('greeting_agent' in globals() and greeting_agent and\n",
    "    'farewell_agent' in globals() and farewell_agent and\n",
    "    'get_weather_stateful' in globals() and\n",
    "    'block_keyword_guardrail' in globals() and\n",
    "    'block_paris_tool_guardrail' in globals()):\n",
    "\n",
    "    root_agent_model = MODEL_GEMINI_2_0_FLASH\n",
    "\n",
    "    root_agent_tool_guardrail = Agent(\n",
    "        name=\"weather_agent_v6_tool_guardrail\", # New version name\n",
    "        model=root_agent_model,\n",
    "        description=\"Main agent: Handles weather, delegates, includes input AND tool guardrails.\",\n",
    "        instruction=\"You are the main Weather Agent. Provide weather using 'get_weather_stateful'. \"\n",
    "                    \"Delegate greetings to 'greeting_agent' and farewells to 'farewell_agent'. \"\n",
    "                    \"Handle only weather, greetings, and farewells.\",\n",
    "        tools=[get_weather_stateful],\n",
    "        sub_agents=[greeting_agent, farewell_agent],\n",
    "        output_key=\"last_weather_report\",\n",
    "        before_model_callback=block_keyword_guardrail, # Keep model guardrail\n",
    "        before_tool_callback=block_paris_tool_guardrail # <<< Add tool guardrail\n",
    "    )\n",
    "    print(f\"✅ Root Agent '{root_agent_tool_guardrail.name}' created with BOTH callbacks.\")\n",
    "\n",
    "    # --- Create Runner, Using SAME Stateful Session Service ---\n",
    "    if 'session_service_stateful' in globals():\n",
    "        runner_root_tool_guardrail = Runner(\n",
    "            agent=root_agent_tool_guardrail,\n",
    "            app_name=APP_NAME,\n",
    "            session_service=session_service_stateful # <<< Use the service from Step 4/5\n",
    "        )\n",
    "        print(f\"✅ Runner created for tool guardrail agent '{runner_root_tool_guardrail.agent.name}', using stateful session service.\")\n",
    "    else:\n",
    "        print(\"❌ Cannot create runner. 'session_service_stateful' from Step 4/5 is missing.\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ Cannot create root agent with tool guardrail. Prerequisites missing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "28eabbd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting execution using 'await' (default for notebooks)...\n",
      "\n",
      "--- Testing Tool Argument Guardrail ('Paris' blocked) ---\n",
      "--- Turn 1: Requesting weather in New York (expect allowed) ---\n",
      "\n",
      ">>> User Query: What's the weather in New York?\n",
      "--- Callback: block_keyword_guardrail running for agent: weather_agent_v6_tool_guardrail ---\n",
      "--- Callback: Inspecting last user message: 'For context:...' ---\n",
      "--- Callback: Keyword not found. Allowing LLM call for weather_agent_v6_tool_guardrail. ---\n",
      "--- Callback: block_paris_tool_guardrail running for tool 'get_weather_stateful' in agent 'weather_agent_v6_tool_guardrail' ---\n",
      "--- Callback: Inspecting args: {'city': 'New York'} ---\n",
      "--- Callback: City 'New York' is allowed for tool 'get_weather_stateful'. ---\n",
      "--- Callback: Allowing tool 'get_weather_stateful' to proceed. ---\n",
      "--- Tool: get_weather_stateful called for New York ---\n",
      "--- Tool: Reading state 'user_preference_temperature_unit': Fahrenheit ---\n",
      "--- Tool: Generated report in Fahrenheit. Result: {'status': 'success', 'report': 'The weather in New york is sunny with a temperature of 77°F.'} ---\n",
      "--- Tool: Updated state 'last_city_checked_stateful': New York ---\n",
      "--- Callback: block_keyword_guardrail running for agent: weather_agent_v6_tool_guardrail ---\n",
      "--- Callback: Inspecting last user message: 'For context:...' ---\n",
      "--- Callback: Keyword not found. Allowing LLM call for weather_agent_v6_tool_guardrail. ---\n",
      "<<< Agent Response: The weather in New york is sunny with a temperature of 77°F.\n",
      "\n",
      "\n",
      "--- Turn 2: Requesting weather in Paris (expect blocked by tool guardrail) ---\n",
      "\n",
      ">>> User Query: How about Paris?\n",
      "--- Callback: block_keyword_guardrail running for agent: weather_agent_v6_tool_guardrail ---\n",
      "--- Callback: Inspecting last user message: 'How about Paris?...' ---\n",
      "--- Callback: Keyword not found. Allowing LLM call for weather_agent_v6_tool_guardrail. ---\n",
      "--- Callback: block_paris_tool_guardrail running for tool 'get_weather_stateful' in agent 'weather_agent_v6_tool_guardrail' ---\n",
      "--- Callback: Inspecting args: {'city': 'Paris'} ---\n",
      "--- Callback: Detected blocked city 'Paris'. Blocking tool execution! ---\n",
      "--- Callback: Set state 'guardrail_tool_block_triggered': True ---\n",
      "--- Callback: block_keyword_guardrail running for agent: weather_agent_v6_tool_guardrail ---\n",
      "--- Callback: Inspecting last user message: 'How about Paris?...' ---\n",
      "--- Callback: Keyword not found. Allowing LLM call for weather_agent_v6_tool_guardrail. ---\n",
      "<<< Agent Response: I am sorry, weather checks for Paris are currently disabled.\n",
      "\n",
      "\n",
      "--- Turn 3: Requesting weather in London (expect allowed) ---\n",
      "\n",
      ">>> User Query: Tell me the weather in London.\n",
      "--- Callback: block_keyword_guardrail running for agent: weather_agent_v6_tool_guardrail ---\n",
      "--- Callback: Inspecting last user message: 'Tell me the weather in London....' ---\n",
      "--- Callback: Keyword not found. Allowing LLM call for weather_agent_v6_tool_guardrail. ---\n",
      "--- Callback: block_paris_tool_guardrail running for tool 'get_weather_stateful' in agent 'weather_agent_v6_tool_guardrail' ---\n",
      "--- Callback: Inspecting args: {'city': 'London'} ---\n",
      "--- Callback: City 'London' is allowed for tool 'get_weather_stateful'. ---\n",
      "--- Callback: Allowing tool 'get_weather_stateful' to proceed. ---\n",
      "--- Tool: get_weather_stateful called for London ---\n",
      "--- Tool: Reading state 'user_preference_temperature_unit': Fahrenheit ---\n",
      "--- Tool: Generated report in Fahrenheit. Result: {'status': 'success', 'report': 'The weather in London is cloudy with a temperature of 59°F.'} ---\n",
      "--- Tool: Updated state 'last_city_checked_stateful': London ---\n",
      "--- Callback: block_keyword_guardrail running for agent: weather_agent_v6_tool_guardrail ---\n",
      "--- Callback: Inspecting last user message: 'Tell me the weather in London....' ---\n",
      "--- Callback: Keyword not found. Allowing LLM call for weather_agent_v6_tool_guardrail. ---\n",
      "<<< Agent Response: The weather in London is cloudy with a temperature of 59°F.\n",
      "\n",
      "\n",
      "--- Inspecting Final Session State (After Tool Guardrail Test) ---\n",
      "Tool Guardrail Triggered Flag: True\n",
      "Last Weather Report: The weather in London is cloudy with a temperature of 59°F.\n",
      "\n",
      "Temperature Unit: Fahrenheit\n"
     ]
    }
   ],
   "source": [
    "# @title 3. Interact to Test the Tool Argument Guardrail\n",
    "import asyncio # Ensure asyncio is imported\n",
    "\n",
    "# Ensure the runner for the tool guardrail agent is available\n",
    "if 'runner_root_tool_guardrail' in globals() and runner_root_tool_guardrail:\n",
    "    # Define the main async function for the tool guardrail test conversation.\n",
    "    # The 'await' keywords INSIDE this function are necessary for async operations.\n",
    "    async def run_tool_guardrail_test():\n",
    "        print(\"\\n--- Testing Tool Argument Guardrail ('Paris' blocked) ---\")\n",
    "\n",
    "        # Use the runner for the agent with both callbacks and the existing stateful session\n",
    "        # Define a helper lambda for cleaner interaction calls\n",
    "        interaction_func = lambda query: call_agent_async(query,\n",
    "                                                         runner_root_tool_guardrail,\n",
    "                                                         USER_ID_STATEFUL, # Use existing user ID\n",
    "                                                         SESSION_ID_STATEFUL # Use existing session ID\n",
    "                                                        )\n",
    "        # 1. Allowed city (Should pass both callbacks, use Fahrenheit state)\n",
    "        print(\"--- Turn 1: Requesting weather in New York (expect allowed) ---\")\n",
    "        await interaction_func(\"What's the weather in New York?\")\n",
    "\n",
    "        # 2. Blocked city (Should pass model callback, but be blocked by tool callback)\n",
    "        print(\"\\n--- Turn 2: Requesting weather in Paris (expect blocked by tool guardrail) ---\")\n",
    "        await interaction_func(\"How about Paris?\") # Tool callback should intercept this\n",
    "\n",
    "        # 3. Another allowed city (Should work normally again)\n",
    "        print(\"\\n--- Turn 3: Requesting weather in London (expect allowed) ---\")\n",
    "        await interaction_func(\"Tell me the weather in London.\")\n",
    "\n",
    "    # --- Execute the `run_tool_guardrail_test` async function ---\n",
    "    # Choose ONE of the methods below based on your environment.\n",
    "\n",
    "    # METHOD 1: Direct await (Default for Notebooks/Async REPLs)\n",
    "    # If your environment supports top-level await (like Colab/Jupyter notebooks),\n",
    "    # it means an event loop is already running, so you can directly await the function.\n",
    "    print(\"Attempting execution using 'await' (default for notebooks)...\")\n",
    "    await run_tool_guardrail_test()\n",
    "\n",
    "    # METHOD 2: asyncio.run (For Standard Python Scripts [.py])\n",
    "    # If running this code as a standard Python script from your terminal,\n",
    "    # the script context is synchronous. `asyncio.run()` is needed to\n",
    "    # create and manage an event loop to execute your async function.\n",
    "    # To use this method:\n",
    "    # 1. Comment out the `await run_tool_guardrail_test()` line above.\n",
    "    # 2. Uncomment the following block:\n",
    "    \"\"\"\n",
    "    import asyncio\n",
    "    if __name__ == \"__main__\": # Ensures this runs only when script is executed directly\n",
    "        print(\"Executing using 'asyncio.run()' (for standard Python scripts)...\")\n",
    "        try:\n",
    "            # This creates an event loop, runs your async function, and closes the loop.\n",
    "            asyncio.run(run_tool_guardrail_test())\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Inspect final session state after the conversation ---\n",
    "    # This block runs after either execution method completes.\n",
    "    # Optional: Check state for the tool block trigger flag\n",
    "    print(\"\\n--- Inspecting Final Session State (After Tool Guardrail Test) ---\")\n",
    "    # Use the session service instance associated with this stateful session\n",
    "    final_session = await session_service_stateful.get_session(app_name=APP_NAME,\n",
    "                                                         user_id=USER_ID_STATEFUL,\n",
    "                                                         session_id= SESSION_ID_STATEFUL)\n",
    "    if final_session:\n",
    "        # Use .get() for safer access\n",
    "        print(f\"Tool Guardrail Triggered Flag: {final_session.state.get('guardrail_tool_block_triggered', 'Not Set (or False)')}\")\n",
    "        print(f\"Last Weather Report: {final_session.state.get('last_weather_report', 'Not Set')}\") # Should be London weather if successful\n",
    "        print(f\"Temperature Unit: {final_session.state.get('user_preference_temperature_unit', 'Not Set')}\") # Should be Fahrenheit\n",
    "        # print(f\"Full State Dict: {final_session.state}\") # For detailed view\n",
    "    else:\n",
    "        print(\"\\n❌ Error: Could not retrieve final session state.\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n⚠️ Skipping tool guardrail test. Runner ('runner_root_tool_guardrail') is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d0f8a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "google-adk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
